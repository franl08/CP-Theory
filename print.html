<!DOCTYPE HTML>
<html lang="pt" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Teóricas de CP</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="chapter_1.html"><strong aria-hidden="true">1.</strong> Apresentação da UC</a></li><li class="chapter-item expanded "><a href="chapter_2.html"><strong aria-hidden="true">2.</strong> Otimização de Código Sequencial</a></li><li class="chapter-item expanded "><a href="chapter_3.html"><strong aria-hidden="true">3.</strong> Hierarquia de Memória</a></li><li class="chapter-item expanded "><a href="chapter_4.html"><strong aria-hidden="true">4.</strong> Paralelismo de Dados e Multithreading em Uniprocessadores</a></li><li class="chapter-item expanded "><a href="chapter_5.html"><strong aria-hidden="true">5.</strong> Aceleradores de Computação: GPU e CUDA</a></li><li class="chapter-item expanded "><a href="chapter_6.html"><strong aria-hidden="true">6.</strong> Introdução à Programação com Memória Partilhada</a></li><li class="chapter-item expanded "><a href="chapter_7.html"><strong aria-hidden="true">7.</strong> Programação Paralela com Memória Partilhada</a></li><li class="chapter-item expanded "><a href="chapter_8.html"><strong aria-hidden="true">8.</strong> Medição e Otimização de Desempenho em Memória Partilhada</a></li><li class="chapter-item expanded "><a href="chapter_9.html"><strong aria-hidden="true">9.</strong> Programação em Memória Distribuída com Passagem de Mensagens</a></li><li class="chapter-item expanded "><a href="chapter_10.html"><strong aria-hidden="true">10.</strong> Algoritmos Paralelos (Sorting)</a></li><li class="chapter-item expanded "><a href="chapter_11.html"><strong aria-hidden="true">11.</strong> De Multicore para Manycore</a></li><li class="chapter-item expanded "><a href="chapter_12.html"><strong aria-hidden="true">12.</strong> Sistemas Top HPC nas listas do TOP500</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Teóricas de CP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="teórica-01"><a class="header" href="#teórica-01">Teórica 01</a></h1>
<h2 id="foco-principal-engenharia-de-performance"><a class="header" href="#foco-principal-engenharia-de-performance"><strong>Foco Principal</strong>: Engenharia de <em>Performance</em></a></h2>
<ul>
<li>
<p>Como?</p>
<ul>
<li>Como é que funcionam os novos computadores e o que é que o futuro lhes trará?</li>
<li>Como é que podemos medir a eficiência da execução?</li>
<li>Como é que funcionam os algoritmos? </li>
<li>Como é que podemos organizar estruturas de dados?</li>
</ul>
</li>
<li>
<p>Onde no <em>hardware</em>?</p>
<ul>
<li>Código sequencial com ILP (Parelelismo ao nível das instruções);</li>
<li>Memória hierárquica;</li>
<li>Paralelismo de processos;</li>
<li>Paralelismo em <em>threads</em>;</li>
<li>Aceleradores (GPU, etc...).</li>
</ul>
</li>
</ul>
<p>É necessário apostar em paralelismo (tanto ao nível do código, como ao nível dos dados), pois o ILP não pode continuar a ser melhorado por questões físicas.</p>
<p>Assim, temos de apostar em novos modelos de paralelismo:</p>
<ul>
<li>
<p>Classes de paralelismo em aplicações:</p>
<ul>
<li><strong>DLP</strong>: Paralelismo ao nível dos dados;</li>
<li><strong>TLP</strong>: Paralelismo ao nível das <em>tasks</em>;</li>
</ul>
</li>
<li>
<p>Classes de paralelismo arquitetural:</p>
<ul>
<li><strong>ILP</strong>: Paralelismo ao nível das instruções;</li>
<li><strong>RLP</strong>: Paralelismo ao nível dos pedidos;</li>
<li><strong>GPUs</strong>: Arquiteturas vetoriais e unidades de processamento gráficas;</li>
<li>Paralelismo ao nível das <em>threads</em>.</li>
</ul>
</li>
</ul>
<h2 id="taxonomia-de-flynn"><a class="header" href="#taxonomia-de-flynn">Taxonomia de <em>Flynn</em></a></h2>
<ul>
<li><strong>SISD</strong>: <em>Stream</em> singular de instruções e <em>stream</em> singular de dados;</li>
<li><strong>SIMD</strong>: <em>Stream</em> singular de instruções e múltiplas <em>streams</em> de dados;</li>
<li><strong>MISD</strong>: Múltiplas <em>streams</em> de instruções e <em>stream</em> singular de dados;</li>
<li><strong>MIMD</strong>: Múltiplas <em>streams</em> de instruções e múltiplas <em>streams</em> de dados.</li>
</ul>
<h2 id="como-medir-a-performance"><a class="header" href="#como-medir-a-performance">Como medir a <em>Performance</em></a></h2>
<ul>
<li>
<p><strong>Métricas</strong>:</p>
<ul>
<li><strong>Largura de Banda ou <em>Throughput</em></strong>:
<ul>
<li>Mede a velocidade;</li>
<li>Quantidade de trabalho num intervalo de tempo.</li>
</ul>
</li>
<li><strong>Latência</strong>:
<ul>
<li>Tempo entre o início e o fim de um evento;</li>
<li>&quot;Quanto tempo demora a responder?&quot;.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong><em>Speedup</em> de X relativa a Y</strong>:
\[  \frac{Tempo\ de\ Execucão_{y}}{Tempo\ de\ Execucão_{x}} \]</p>
</li>
<li>
<p><strong>Tempo de Execução</strong>:</p>
<ul>
<li><em>Wall Clock Time</em>:
<ul>
<li>Inclui os <em>overheads</em> do sistema.</li>
</ul>
</li>
<li>CPU Time*:
<ul>
<li>Apenas o tempo de computação do programa.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="princípios-de-design-de-computadores"><a class="header" href="#princípios-de-design-de-computadores">Princípios de <em>design</em> de computadores</a></h2>
<ul>
<li>Tirar vantagem do paralelismo;</li>
<li>Princípio da Localidade:
<ul>
<li>Reutilização de dados e instruções.</li>
</ul>
</li>
<li>Focar-se no caso comum:
<ul>
<li>Lei de <em>Amdahl</em>:
\[ speedup\ overall = \frac{t_{exec}\ antigo}{t_{exec}\ novo} = \frac{1}{\sum{\frac{f_i}{s_i}}}, f_i \equiv fracões\ com\ melhoria; s_{i} \equiv speedup\ de\ cada\ funcão \] </li>
</ul>
</li>
</ul>
<p><strong>Nota</strong>:</p>
<p>Melhorar uma porção do programa 90x não é equivalente a um ganho de 90x no programa.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="teórica-02"><a class="header" href="#teórica-02">Teórica 02</a></h1>
<h2 id="otimização-de-código-sequencial"><a class="header" href="#otimização-de-código-sequencial">Otimização de Código Sequencial</a></h2>
<h3 id="otimização-de-desempenho"><a class="header" href="#otimização-de-desempenho">Otimização de Desempenho</a></h3>
<p><strong>Fases de Desenvolvimento</strong></p>
<ol>
<li>Selecionar o melhor algoritmo;
<ol>
<li>Utilizar a análise de complexidade para comprar algoritmos;</li>
</ol>
</li>
<li>Escrever código legível e fácil de gerir;</li>
<li>Eliminar bloqueadores de otimizações;</li>
<li>Medir o perfil de execução.
<ol>
<li>Otimar as partes críticas para o desempneho
<ol>
<li>Operações repetidas muitas vezes (p.e. ciclos interiores)</li>
</ol>
</li>
</ol>
</li>
</ol>
<ul>
<li>Código com otimizações é mais complexo de ler, manter e de garantir a correção.</li>
</ul>
<h2 id="paralelismo-ao-nível-das-instruções"><a class="header" href="#paralelismo-ao-nível-das-instruções">Paralelismo ao nível das Instruções</a></h2>
<ul>
<li>O objetivo é maximizar o CPI.
<ul>
<li><em>Pipeline CPI</em>:
<ul>
<li><em>Pipeline</em> ideal do CPI +</li>
<li><em>Structural Stalls</em> +</li>
<li><em>Data Hazard Stalls</em> + </li>
<li><em>Control Stalls</em></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>\[  CPU_{time} = #Instr * CPI * Clk_{cycle}  \]</p>
<ul>
<li>Paralelismo com bloco básico é limitado
<ul>
<li>Tamanho típico do bloco é de 3 a 6 instruções;</li>
<li>Deve ser otimizado entre <em>branches</em>.</li>
</ul>
</li>
</ul>
<h2 id="dependência-de-dados"><a class="header" href="#dependência-de-dados">Dependência de Dados</a></h2>
<ul>
<li><strong>Paralelismo ao nível dos <em>loops</em></strong>
<ul>
<li>Desenrolar o <em>loop</em> estáticamente o dinâmicamente;</li>
<li>Utilizar o SIMD (processadores vetoriais e GPUs).</li>
</ul>
</li>
<li><em><strong>Data Hazards</strong></em>
<ul>
<li><em>Read After Write</em> (RAW);</li>
<li><em>Write After Write</em> (WAW);</li>
<li><em>Write After Read</em> (WAR).</li>
</ul>
</li>
<li><strong>Dependências de Controlo</strong>
<ul>
<li>Ordenação da instrução \( i \) respeitando a instrução da <em>branch</em>.</li>
</ul>
</li>
</ul>
<h2 id="previsão-da-branch"><a class="header" href="#previsão-da-branch">Previsão da <em>Branch</em></a></h2>
<ul>
<li><em><strong>Basic 2-bit predictor</strong></em>:
<ul>
<li>Para cada <em>branch</em>:
<ul>
<li>Prevê se o salto será dado ou não;</li>
<li>Se a previsão estiver errada 2 vezes consecutivas, altera-a.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Previsão de correlação</strong>:
<ul>
<li>Múltiplos <em>2-bit predictors</em> para cada <em>branch</em>;</li>
<li>Um para cada combinação possível de <em>outcomes</em> das \( n \) <em>branches</em> precedentes.</li>
</ul>
</li>
<li><strong>Agendamento Dinâmico</strong>:
<ul>
<li>Reordena as intruções para reduzir <em>stalls</em> enquanto mantém o <em>flow</em> de dados.</li>
</ul>
</li>
<li><strong>Especulação Baseada em <em>Hardware</em></strong>:
<ul>
<li>Executa instruções de acordo com a execução prevista, mas apenas apresenta os resultados no caso da previsão ser correta.</li>
</ul>
</li>
</ul>
<h2 id="multiple-issue"><a class="header" href="#multiple-issue"><em>Multiple Issue</em></a></h2>
<ul>
<li>Uma forma de reduzir o CPI para &lt; 1.</li>
</ul>
<p><img src="images/cpi.png" alt="image Multiple Issue" /></p>
<h2 id="otimizações-comuns-de-compiladores"><a class="header" href="#otimizações-comuns-de-compiladores">Otimizações Comuns de Compiladores</a></h2>
<ul>
<li><em><strong>Loops</strong></em>
<ul>
<li>Identifica as variáveis de indução que são incrementadas/decrementadas por um valor fixo em cada iteração do ciclo (p.e. <code>j = i * 4 + 1</code> passa a <code>j += 5</code>);</li>
<li><strong>Fissão</strong>: parte o <em>loop</em> em múltiplos <em>loops</em>, sendo que cada um será responsável por apenas parte do corpo do <em>loop</em> original;</li>
<li><strong>Fusão</strong>: combina múltiplos <em>loops</em> de forma a reduzir o <em>overhead</em>;</li>
<li><strong>Inversão</strong>: altera o <em>while loop</em> genérico para um <em>do-while</em>;</li>
<li><strong>Intercâmbio</strong>: troca os <em>loops</em> internos com os <em>loops</em> externos;</li>
<li>Movimenta o invariante do <em>loop</em> de forma a melhorar o desempenho do programa;</li>
<li><em><strong>Loop Unrolling</strong></em>: duplica o corpo do <em>loop</em> múltiplas vezes;</li>
<li><em><strong>Loop Splitting</strong></em>: parte o <em>loop</em> em múltiplos <em>loops</em> com o mesmo corpo, no entanto, estes iteram em partes contíguas dentro dos limites do indíce pretendido.</li>
</ul>
</li>
<li><em><strong>Data Flow</strong></em>
<ul>
<li>Elimina/Partilha subexpressões comuns;</li>
<li>Reduz os custos das operações, isto é, as operações muito custosas são substituídas por operações mais &quot;baratas&quot;;</li>
<li><em><strong>Constant Folding</strong></em>: substitui expressões constantes (p.e. <code>3 + 4</code>) pelo seu valor final (neste caso, <code>7</code>);</li>
<li><em><strong>Dead Store Elimination</strong></em>: remove variáveis atribuídas que não voltarão a ser lidas.</li>
</ul>
</li>
<li><em><strong>Code Generation</strong></em>
<ul>
<li><strong>Alocação de Registos</strong>: as variáveis mais utilizadas são mantidas em registos do processador;</li>
<li><strong>Seleção de Instruções</strong>: seleciona 1 forma de múltiplas disponíveis para efetuar uma operação;</li>
<li><strong>Agendamento de Instruções</strong>: evita <em>pipeline stalls</em>;</li>
<li><strong>Re-materialização</strong>: Recalcula um valor, ao invés de o ir buscar à memória.</li>
</ul>
</li>
<li><strong>Outras</strong>
<ul>
<li><em><strong>Bounds-checking elimination</strong></em>;</li>
<li><strong>Reordenação de blocos de código</strong>: altera a ordem de blocos básicos;</li>
<li><strong>Eliminição de código inutilizado</strong>;</li>
<li><em><strong>Inline Expansion</strong></em>: insere o corpo de uma função na chamada à função.</li>
</ul>
</li>
<li><strong>Limitações</strong>
<ul>
<li><em>Memory aliasing</em> e efeitos secundários das funções;</li>
<li>Tipicamente, os compiladores não tentam melhorar a complexidade dos algoritmos;</li>
<li>Tipicamente, os compiladores apenas lidam com uma parte do programa de cada vez;</li>
<li>Pode gerar-se um <em>overhead</em> no tempo de compilação devido às otimizações do compilador.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hierarquia-de-memória"><a class="header" href="#hierarquia-de-memória">Hierarquia de Memória</a></h1>
<h2 id="hiato-processador-memória"><a class="header" href="#hiato-processador-memória">Hiato Processador-Memória</a></h2>
<ul>
<li>Para cada instrução, deve ser feito o seguinte processo:
<ul>
<li>Ler Instrução;</li>
<li>Ler Operando;</li>
<li>Escrever Resultado.</li>
</ul>
</li>
<li>O <strong>hiato processador-memória</strong> diz-nos:
<ul>
<li>&quot;A memória é incapaz de alimentar o processador com instruções e dados com uma taxa suficiente de forma a mantê-lo constantemente ocupado&quot;.</li>
</ul>
</li>
<li>A causa para este problema é a diferente taxa de aumento do desempenho entre os processadores e a memória nos últimos anos;</li>
<li>Trata-se de um dos principais obstáculos à melhoria do desempenho dos sistemas de computação.</li>
</ul>
<h2 id="princípio-da-localidade"><a class="header" href="#princípio-da-localidade">Princípio da Localidade</a></h2>
<ul>
<li>Permite acelerar os acessos à memória através de uma hierarquia;
<ul>
<li>&quot;Os programas tendem a aceder a uma porção limitada de memória num dado período de tempo&quot;.</li>
</ul>
</li>
<li>Permite utilizar memória mais rápida para armazenar a informação usada mais frequentemente/recentemente;</li>
<li>Permite tirar partido da largura de banda, uma vez que a informação transferida entre diferentes níveis da hierarquia é efetuada por blocos.</li>
</ul>
<h3 id="localidade-temporal"><a class="header" href="#localidade-temporal">Localidade Temporal</a></h3>
<p>Um elemento de memória acedido pelo processador será, com grande probabilidade, acedido de novo num futuro próximo;</p>
<ul>
<li><strong>Exemplos</strong>:
<ul>
<li>Tanto as instruções dentro dos ciclos, como as variáveis usadas como contadores de ciclos, são acedidas repetidamente em curtos intervalos de tempo.</li>
</ul>
</li>
<li><strong>Consequência</strong>:
<ul>
<li>A 1ª vez que um elemento de memória é acedido deve ser lido do nível mais baixo (p.e. da memória central);</li>
<li>Da 2ª vez que é acedido, no entanto, é muito provável que este se encontre em <em>cache</em>, evitando-se assim o tempo de leitura da memória central.</li>
</ul>
</li>
</ul>
<h3 id="localidade-espacial"><a class="header" href="#localidade-espacial">Localidade Espacial</a></h3>
<p>Se um elemento de memória é acedido pelo CPU, então elementos com endereços na sua proximidade serão, com grande probabilidade, acedidos num futuro próximo.</p>
<ul>
<li><strong>Exemplos</strong>:
<ul>
<li>As instruções do programa são, normalmente, acedidas em sequência, assim como, na maior parte dos programas, os elementos de vetores/matrizes.</li>
</ul>
</li>
<li><strong>Consequência</strong>:
<ul>
<li>A 1ª vez que um elemento de memória é acedido, deve ser lido do nível mais baixo (p.e. memória central), no entanto, não será lido sozinho, mas sim com um bloco de elementos com endereços na sua vizinhança.</li>
<li>Se o processador, nos próximos ciclos, aceder a um endereço vizinho do anterior (p.e. próxima instrução ou próximo elemento de um vetor), a probabiliade desse elemento já estar em <em>cache</em> é elevada.</li>
</ul>
</li>
</ul>
<h3 id="inclusão"><a class="header" href="#inclusão">Inclusão</a></h3>
<ul>
<li>Os dados contidos num nível mais próximo do processador são um subconjunto dos dados contidos no nível anterior;</li>
<li>O nível mais baixo contém a totalidade dos dados;</li>
<li>Os dados são copiados entre níveis em blocos.</li>
</ul>
<p><img src="images/hierarquia_mem.png" alt="image Níveis hierárquicos" /></p>
<h3 id="terminologia"><a class="header" href="#terminologia">Terminologia</a></h3>
<ul>
<li><strong>Linha</strong>: A <em>cache</em> encontra-se dividida em linhas. Cada linha terá o seu endereço (índice) e tem a capacidade de um bloco;</li>
<li><strong>Bloco</strong>: Quantidade de informação que é transferida de cada vez da memória central para a <em>cache</em> (ou entre níveis de <em>cache</em>). É igual à capacidade da linha;</li>
<li><em><strong>Hit</strong></em>: Diz-se que ocorreu um <em>hit</em> quando o elemento de memória acedido pelo CPU se encontra em <em>cache</em>;</li>
<li><em><strong>Miss</strong></em>: Diz-se que ocorreu um <em>miss</em> quando o elemento de memória acedido pela CPU não se encontra em <em>cache</em>, sendo necessário lê-lo do nível inferior da hierarquia.</li>
<li><em><strong>Hit Rate</strong></em>: Percentagem de <em>hits</em> ocorridos relativamente ao total de acessos à memória;</li>
<li><em><strong>Miss Rate</strong></em>: Percentagem de <em>misses</em> ocorridos relativamente ao total de acessos à memória. (\( Miss\ Rate\ = 1 - hit\ rate \));</li>
<li><em><strong>Hit Time</strong></em>: Tempo necessário para aceder à <em>cache</em>, incluindo o tempo necessário para determinar se o elemento a que o CPU está a aceder se encontra, ou não, em <em>cache</em>;</li>
<li><em><strong>Miss Penalty</strong></em>: Tempo necessário para carregar um bloco da memória central (ou de um nível inferior) para a <em>cache</em> quando ocorre um <em>miss</em>.</li>
</ul>
<h2 id="causas-de-misses"><a class="header" href="#causas-de-misses">Causas de <em>Misses</em></a></h2>
<ul>
<li>Os <strong>3 C's</strong>:
<ul>
<li><strong>Obrigatória</strong>;
<ul>
<li>Primeira referência ao bloco.</li>
</ul>
</li>
<li><strong>Capacidade</strong>;
<ul>
<li>Blocos que são descartados e mais tarde são necessários.</li>
</ul>
</li>
<li><strong>Conflito</strong>;
<ul>
<li>O programa faz repetidas referências a endereços de diferentes blocos de forma a mapear a mesma localização em <em>cache</em>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Coerência</strong>:
<ul>
<li>Diferentes processadores devem ver o mesmo valor na mesma localização;</li>
<li>Duas fontes: <em>true-sharing</em> e <em>false-sharing</em></li>
</ul>
</li>
</ul>
<h2 id="multilevel-caches"><a class="header" href="#multilevel-caches"><em>Multilevel Caches</em></a></h2>
<p>De forma a evitar o máximo de <em>misses</em> possível, opta-se por uma arquitetura de <em>caches multilevel</em>. Assim, tem-se os seguintes níveis:</p>
<ul>
<li><em>Cache</em> primária presa à CPU;
<ul>
<li>Muito pequena, mas rápida.</li>
</ul>
</li>
<li><em>Cache</em> de nível 2;
<ul>
<li>Serve os <em>misses</em> da <em>cache</em> de nível 1;</li>
<li>Maior que a anterior, mas mais lenta, no entanto, mais rápida que a seguinte ou que a memória principal.</li>
</ul>
</li>
<li><em>Cache</em> de nível 3 (em alguns casos, não existe e poderá já ser a memória principal);
<ul>
<li>Serve os <em>misses</em> da <em>cache</em> de nível 2;</li>
<li>Maior que a anterior, mas mais lenta, sendo mais rápida que a memória principal.</li>
</ul>
</li>
<li>Memória Principal.
<ul>
<li>Serve os <em>misses</em> da <em>cache</em> de nível 3 ou, caso esta não exista, os <em>misses</em> da <em>cache</em> de nível 2.</li>
</ul>
</li>
</ul>
<p><img src="images/3_level_cache_examples.png" alt="image Exemplos" /></p>
<h3 id="desempenho"><a class="header" href="#desempenho">Desempenho</a></h3>
<p>\[ CPU_{exec-time}=(CPU_{clock-cycles} + Mem_{stall-cycles}) \times Clock\ cycle\ time \]
\[ CPU_{exec-time}=(IC \times CPI_{CPU} + \frac{Mem_{stall-cycles}}{Instr}) \times Clock\ cycle\ time \]</p>
<ul>
<li>Com a introdução de uma <em>cache single-level</em>:
\[ Mem_{stall-cycles} = IC \times \dots Miss\ rate \dots Mem\ accesses\ \dots Miss\ penalty \dots \]
\[ Mem_{stall-cycles} = IC \times \frac{Misses}{Instruction} \times Miss\ Penalty \]
\[ \frac{Misses}{Instruction} = \frac{Miss\ rate \times Memory\ accesses}{Instruction\ Count} = Miss\ Rate \times \frac{Memory\ Accesses}{Instruction} \]</li>
</ul>
<p>Para cada nível \( i \) adicional de <em>cache</em>:</p>
<p>\[ Mem_{accesses}<em>{level</em>{i}} = \frac{Misses}{Instruction_{level_{i - 1}}} \]
\[ Miss_{penalty}<em>{level</em>{i}} = (Hit\ rate \times Hit\ time \times Miss\ rate \times Miss\ penalty)<em>{level</em>{i + 1}} \]</p>
<h3 id="miss-rates"><a class="header" href="#miss-rates"><em>Miss Rates</em></a></h3>
<ul>
<li>
<p><em>Local Miss Rate</em></p>
<ul>
<li>Dado pelo número de <em>misses</em> na <em>cache</em> dividido pelo total de acessos à <em>cache</em>;</li>
<li>Para o primeiro nível será igual ao \( Miss\ rate_{L1} \) e para o segundo será \( Miss\ rate_{L2} \).</li>
</ul>
</li>
<li>
<p><em>Global Miss Rate</em></p>
<ul>
<li>Dado pelo número de <em>misses</em> na <em>cache</em> dividido pelo total de acessos à memória gerados pela PU;</li>
<li>Para o primeiro nível será igual ao \( Miss\ rate_{L1} \), no entanto, para o segundo já será \( Miss\ rate_{L1} \times Miss\ rate_{L2} \).</li>
</ul>
</li>
</ul>
<h2 id="desempenho-1"><a class="header" href="#desempenho-1">Desempenho</a></h2>
<p>Como é que a hierarquia de memória influencia o \( T_{exec} \)?</p>
<ul>
<li>Cada acesso à memória irá originar ciclos adicionais na execução do programa (\(  \#CC_{MEM} \)) devido aos <em>misses</em>:</li>
</ul>
<p>\[ T_{exec} = ( \#CC +  \#CC_{MEM}) \times T_{CC} \]</p>
<ul>
<li>Cada <em>miss</em> implicará um aumento do \(  \#CC \) em ciclos de <em>miss penalty</em>, logo:</li>
</ul>
<p>\[  \#CC_{MEM} = no.\ miss \times miss\ penalty \]</p>
<p>Sendo que, o \( no.\ miss \) será dado por:</p>
<p>\[ miss\ rate \times no.\ acessos\ mem \]</p>
<ul>
<li>Assim, visto que \(  \#CC =  \#I \times CPI \), temos que:</li>
</ul>
<p>\[ T_{exec} =  \#I \times (CPI_{CPU} + CPI_{MEM}) \times T_{CC} \]</p>
<ul>
<li>
<p>Em que:</p>
<ul>
<li>\( CPI_{CPU} \): número de ciclos que o processador precisa, em média, para executar cada instrução;</li>
<li>\( CPI_{MEM} \): número de ciclos que o processador precisa de parar, em média, para esperar por dados da memória, visto que não foi capaz de encontrar estes em <em>cache</em>. Vulgarmente, designam-se por <em>memory stall cycles</em> ou <em>wait states</em>.</li>
</ul>
</li>
<li>
<p>Podemos calcular o \( CPI_{MEM} \) da seguinte forma:</p>
</li>
</ul>
<p>\[ CPI_{MEM} = \% acessos\ Mem \times miss\ rate \times miss\ penalty \]</p>
<ul>
<li>De notar que os acessos à memória devem-se a:
<ul>
<li>Acesso a dados (instruções de <em>Load</em> ou <em>Store</em>);</li>
<li>Busca de instruções.</li>
</ul>
</li>
<li>Como estes têm comportamentos diferentes, usam-se percentagens diferentes:
<ul>
<li><strong>Dados</strong>: Apenas uma determinada percentagem de instruções irá aceder à memória (\( \%Mem \)), pelo que, \( missrate_D \) referir-se-á ao acesso a dados;</li>
<li><strong>Instruções</strong>: Todas as instruções são lidas da memória, logo a percentagem de acesso à memória será de 100%, \( missrate_I \) referir-se-á ao acesso às instruções;</li>
<li>Geralmente, a \( missrate_I \) é menor que a \( missrate_D \) devido à utilização da localidade espacial.</li>
</ul>
</li>
<li>Temos, então:</li>
</ul>
<p>\[ CPI_{MEM} = (missrate_I + \% Mem \times missrate_D) \times misspenalty \]</p>
<h2 id="coerência-na-cache"><a class="header" href="#coerência-na-cache">Coerência na <em>Cache</em></a></h2>
<ul>
<li><strong>Coerência</strong>
<ul>
<li>Todas as leituras de um processador devem retornar o valor escrito mais recentemente;</li>
<li>Escritas para a mesma localização por dois processadores devem ser vistas na mesma ordem por todos os processadores;</li>
<li>Ou seja, a coerência deverá definir o comportamento para escritas e leituras na mesma localização da memória.</li>
</ul>
</li>
<li><strong>Consistência</strong>
<ul>
<li>Ocorre quando um valor escrito deverá ser devolvido por uma leitura;</li>
<li>Se um processador escrever numa localização A e, posteriormente, numa localização B, qualquer processador que vir o novo valor de B, deverá também ver o valor de A;</li>
<li>Ou seja, a consistência deverá definir o comportamento para escritas e leituras respeitando o acesso a outras localizações de memórias.</li>
</ul>
</li>
</ul>
<h3 id="forçar-a-coerência"><a class="header" href="#forçar-a-coerência">Forçar a Coerência</a></h3>
<ul>
<li>Uma <em>cache</em> coerente providencia:
<ul>
<li>Migrações: movimentos de dados;</li>
<li>Replicações: múltiplas cópias dos dados.</li>
</ul>
</li>
<li>Protocolos para implementar coerência em <em>caches</em>:
<ul>
<li><em>Directory Based</em>
<ul>
<li>Manter o estado partilhado de cada bloco numa única localização;</li>
</ul>
</li>
<li><em>Snooping</em>
<ul>
<li>Cada <em>core</em> deverá seguir o estado de partilha de cada bloco.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="otimizações-básicas"><a class="header" href="#otimizações-básicas">Otimizações Básicas</a></h2>
<ul>
<li>Maior tamanho dos blocos;
<ul>
<li>Reduz o número de <em>misses</em> obrigatórias;</li>
<li>Aumenta a capacidade e as <em>misses</em> por conflito, bem como a <em>miss penalty</em>.</li>
</ul>
</li>
<li>Aumentar o tamanho total da <em>cache</em> para reduzir a <em>miss rate</em>;
<ul>
<li>Aumenta o <em>hit time</em> e o consumo energético.</li>
</ul>
</li>
<li>Maior associatividade;
<ul>
<li>Reduz as <em>misses</em> por conflito;</li>
<li>Aumenta o <em>hit time</em> e o consumo energético.</li>
</ul>
</li>
<li>Maior número de níveis de <em>cache</em>;
<ul>
<li>Reduz o tempo médio de acesso à memória.</li>
</ul>
</li>
<li>Dar prioridade a <em>misses</em> de leitura ao invés de <em>misses</em> de escrita;
<ul>
<li>Reduz a <em>miss penalty</em>.</li>
</ul>
</li>
<li>Evitar a tradução de endereços ao fazer indexação na <em>cache</em>:
<ul>
<li>Reduz o <em>hit time</em>.</li>
</ul>
</li>
</ul>
<h2 id="tecnologias-de-memória-e-otimizações"><a class="header" href="#tecnologias-de-memória-e-otimizações">Tecnologias de Memória e Otimizações</a></h2>
<ul>
<li>Métricas de desempenho:
<ul>
<li>Latência é uma preocupação da <em>cache</em>;</li>
<li>Largura de banda é uma preocupação dos multi-processadores e I/O;</li>
<li>Tempo de acesso;
<ul>
<li>Tempo entre um pedido de leitura e a chegada da <em>word</em> pretendida.</li>
</ul>
</li>
<li>Tempo de ciclo.
<ul>
<li>Tempo mínimo entre pedidos não relacionados à memória.</li>
</ul>
</li>
</ul>
</li>
<li>A memória <em>SRAM</em> tem uma baixa latência, pelo que é utilizada para <em>cache</em>;</li>
<li>Organizando os <em>chips</em> de <em>DRAM</em> em diversas pilhas providenciará uma grande largura de banda que deve ser utilizada para a memória principal.</li>
<li><strong>SRAM</strong>:
<ul>
<li>Precisa de pouca energia para reter o <em>bit</em>;</li>
<li>Precisa de 6 transístores por <em>bit</em>.</li>
</ul>
</li>
<li><strong>DRAM</strong>:
<ul>
<li>Deve ser reescrita depois de lida;</li>
<li>Deve ser atualizada de forma periódica;
<ul>
<li>+/- 8ms (cerca de 5% do tempo);</li>
<li>Cada linha pode ser atualizada em simultâneo.</li>
</ul>
</li>
<li>Um transístor por <em>bit</em>;</li>
<li>As linhas de endereços são multiplexadas.
<ul>
<li>Metade superior do endereço: <em>row access strobe</em> (RAS);</li>
<li>Metade inferior do endereço: <em>column access strobe</em> (CAS).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="memória-flash"><a class="header" href="#memória-flash">Memória <em>Flash</em></a></h2>
<ul>
<li>Tipo de EEPROM;</li>
<li>Tipos:
<ul>
<li><strong>NAND</strong> (mais densa);</li>
<li><strong>NOR</strong> (mais rápida).</li>
</ul>
</li>
</ul>
<h2 id="otimizações-avançadas"><a class="header" href="#otimizações-avançadas">Otimizações Avançadas</a></h2>
<ul>
<li>Reduzir o <em>hit time</em>;
<ul>
<li><em>Caches</em> de primeiro nível mais pequenas e simples;</li>
<li>Implementação de <em>way predict</em>.
<ul>
<li>Especulativamente, seleciona um caminho dos disponíveis antes de iniciar um acesso normal à <em>cache</em>. Ao seguir apenas o caminho previsto, em vez de todos os disponíveis, poupará a nível energético.</li>
</ul>
</li>
</ul>
</li>
<li>Aumentar a largura de banda;
<ul>
<li>Implementação de <em>pipelined caches</em>, <em>multibanked caches</em>, <em>non-blocking caches</em>.</li>
</ul>
</li>
<li>Reduzir a <em>miss penalty</em>;
<ul>
<li>Ter, primeiramente, a <em>critical word</em>, juntar os <em>buffers</em> de escrita.</li>
</ul>
</li>
<li>Reduzir a <em>miss rate</em>;
<ul>
<li>Otimizações do compilador.</li>
</ul>
</li>
<li>Reduzir a <em>miss penalty</em> ou a <em>miss rate</em> através de paralelismo.
<ul>
<li><em>Hardware</em> ou compilador fazem <em>prefetching</em>.</li>
</ul>
</li>
</ul>
<h2 id="resumo"><a class="header" href="#resumo">Resumo</a></h2>
<p><img src="images/summary_memhier.png" alt="image Resumo" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="paralelismo-de-dados-e-multithreading-em-uniprocessadores"><a class="header" href="#paralelismo-de-dados-e-multithreading-em-uniprocessadores">Paralelismo de Dados e <em>Multithreading</em> em Uniprocessadores</a></h1>
<h2 id="taxonomia-de-flynn-1"><a class="header" href="#taxonomia-de-flynn-1">Taxonomia de Flynn</a></h2>
<p><img src="images/flynn_tax.png" alt="image Taxonomia de Flynn" /></p>
<ul>
<li><strong>SISD</strong>: <em>Single Instruction, Single Data</em>;</li>
<li><strong>SIMD</strong>: <em>Single Instruction, Multiple Data</em>;</li>
<li><strong>MISD</strong>: <em>Multiple Instruction, Single Data</em>;</li>
<li><strong>MIMD</strong>: <em>Multiple Instruction, Multiple Data</em>;</li>
<li><strong>SPMD</strong>: <em>Single Program, Multiple Data</em>:
<ul>
<li>Programa paralelo num computador MIMD;</li>
<li>Precisa de código condicional para diferentes processadores.</li>
</ul>
</li>
</ul>
<h2 id="introdução"><a class="header" href="#introdução">Introdução</a></h2>
<ul>
<li>Arquiteturas SIMD permitem que se explore, de forma significativa, paralelismo ao nível dos dados para:
<ul>
<li>Computação científica orientada a matrizes;</li>
<li>Processamento de mídias;</li>
<li>Algoritmos de <em>Machine Learning</em>.</li>
</ul>
</li>
<li>Além disso, estas arquiteturas são mais eficientes que as MIMD a nível energético;
<ul>
<li>Precisam de dar <em>fetch</em> a uma única instrução por operação de dados;</li>
<li>Isto torna estas arquiteturas atrativas para dispositivos móveis pessoais.</li>
</ul>
</li>
<li>Por fim, esta arquitetura ainda permite que o programador tenha um pensamento sequencial.</li>
</ul>
<h2 id="paralelismo-simd"><a class="header" href="#paralelismo-simd">Paralelismo SIMD</a></h2>
<ul>
<li>Arquiteturas vetoriais;</li>
<li>Extensões SIMD;
<ul>
<li>É uma extensão ao <em>instruction set</em> do SIMD para arquiteturas x86 desenhada pela Intel;</li>
<li>Contém 70 novas instruções, sendo que grande parte trabalha com dados de <em>single precision floating-points</em>;</li>
<li>Estas instruções (e as instruções do SIMD), têm um grande ganho de desempenho quando efetuam exatamente as mesmas operações em múltiplos objetos de dados. Geralmente, são utilizadas para processamento de sinais digitais e para processamento gráfico.</li>
</ul>
</li>
<li>GPUs;</li>
<li>Para processadores x86:
<ul>
<li>Expectável que se tenha 2 <em>cores</em> adicionais por ano;</li>
<li>A largura é expectável que duplique a cada 4 anos;</li>
<li>Potencial ganho de velocidade: 2x a velocidade do MIMD.</li>
</ul>
</li>
</ul>
<h3 id="arquiteturas-vetoriais"><a class="header" href="#arquiteturas-vetoriais">Arquiteturas Vetoriais</a></h3>
<ul>
<li>Ideia base:
<ul>
<li>Lê conjuntos de elementos de dados (da memória) para registos vetoriais;</li>
<li>Efetua operações sobre esses registos;</li>
<li>Dispersa os resultados obtidos de volta à memória (<em>scatter</em>).</li>
</ul>
</li>
<li>Os registos são controlados pelo compilador.
<ul>
<li>Utilizado para esconder a latência da memória;</li>
<li>Alavanca a largura de banda da memória.</li>
</ul>
</li>
</ul>
<h4 id="vmips"><a class="header" href="#vmips">VMIPS</a></h4>
<ul>
<li>Exemplo: RV64V
<ul>
<li>Tem 32 registos vetoriais de 64 <em>bits</em>;
<ul>
<li>O ficheiro de registo tem 16 portas de leitura e 8 de escrita.</li>
</ul>
</li>
<li>Unidades funcionais vetoriais;
<ul>
<li>Completamente em <em>pipeline</em>;</li>
<li>Deteta dependências de dados e de controlo.</li>
</ul>
</li>
<li>Unidade vetorial de <em>load-store</em>;
<ul>
<li>Completamente em <em>pipeline</em>;</li>
<li>Uma <em>word</em> por <em>clock cycle</em> após a latência inicial.</li>
</ul>
</li>
<li>Registos escalares.
<ul>
<li>31 registos de propósito geral;</li>
<li>32 registos de <em>floating-point</em>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="desafios"><a class="header" href="#desafios">Desafios</a></h4>
<ul>
<li>Tempo de arranque;
<ul>
<li>Latência de uma unidade funcional vetorial é bastante alta.</li>
</ul>
</li>
<li>Melhorias possíveis:
<ul>
<li>Mais do que 1 elemento por <em>clock cycle</em>;</li>
<li><em>Non-64 wide vectors</em>;</li>
<li>Vetorizar o código de <em>if-statements</em>;</li>
<li>Otimizações ao sistema de memória de forma a suportar processadores vetoriais;</li>
<li>Múltiplas matrizes dimensionais (acessos à memória sem passos unitários);</li>
<li>Matrizes dispersas;</li>
<li>Programar de forma específica para um computador vetorial.</li>
</ul>
</li>
</ul>
<h4 id="programação-vetorial"><a class="header" href="#programação-vetorial">Programação Vetorial</a></h4>
<ul>
<li>Os compiladores são elementos chave para dar dicas sobre se uma secção de código irá vetorizar ou não;</li>
<li>Devemos verificar se as iterações entre <em>loops</em> terão depedências de dados e/ou <em>if-statements</em>, pois, caso contrário, a vetorização será comprometida;</li>
<li>O custo de arquiteturas vetoriais é muito elevado, no entanto existem algumas variantes como extensões a processadores escalares, mas:
<ul>
<li>Não suportam acessos não unitários à memória, pelo que se deve ter cuidado na definição de estruturas de dados;</li>
<li>Também não suportam o <em>mask register</em>, <em>gather-scatter</em>, etc...</li>
</ul>
</li>
</ul>
<h3 id="extensões-a-simd"><a class="header" href="#extensões-a-simd">Extensões a SIMD</a></h3>
<ul>
<li>Aplicações de mídia operam em tipos de dados mais pequenos que tamanho nativo de uma <em>word</em>;</li>
<li>Limitações comparadas a arquiteturas vetoriais:
<ul>
<li>Número de operando de dados codificado no <em>op code</em>;</li>
<li>Não há modos de endereçamento sofisticados;</li>
<li>Não há <em>mask register</em>.</li>
</ul>
</li>
</ul>
<h3 id="implementações-simd"><a class="header" href="#implementações-simd">Implementações SIMD</a></h3>
<ul>
<li>Da Intel:
<ul>
<li>MMX (1996);</li>
<li><em>Streaming SIMD Extensions</em> (SSE) (1999);</li>
<li><em>Advanced Vector eXtensions</em> (AVX) (2010...);
<ul>
<li>Operações em 8 <em>32-bit fp</em> ou 4 <em>64-bit fp</em> (inteiros apenas no AVX-2);</li>
<li>Largura de <em>512-bits</em> no AVX-512 (e no Larrabee e Phi-KNC).</li>
</ul>
</li>
<li>Os operandos têm de estar em localizações de memória consecutivas e alinhadas!</li>
</ul>
</li>
<li>AMD Zen/Epyc: semelhante ao AVX-2;</li>
<li>Arquitetura ARMv8 (64-bit): NEON e SVE.</li>
</ul>
<h2 id="extensões-vetoriais"><a class="header" href="#extensões-vetoriais">Extensões Vetoriais</a></h2>
<ul>
<li>Arquiteturas vetoriais/extensões SIMD são abordagens híbridas:
<ul>
<li>Mistura capacidades de operações (super)escalares + vetoriais num único dispositivo;</li>
<li>Abordagem altamente <em>pipelined</em> de forma a reduzir a <em>penalty</em> de acesso à memória;</li>
<li>Acesso fechado à memória partilhada provoca uma menor latência.</li>
</ul>
</li>
<li>Evolução destas arquiteturas:
<ul>
<li>Aceleradores de computação otimizados para <em>number crunching</em> (GPUs);</li>
<li>Adição do suporte a operações de multiplicação + acumulação para matrizes;
<ul>
<li>Muitas aplicações utilizam computações com matrizes, nomeadamente o <em>dot product</em>;</li>
<li>Os produtores, muitas vezes, chamam a esta extensão <em>Tensor Processing Unit</em> (TPU).</li>
</ul>
</li>
<li>Suporte para <em>half-precision FP</em> e inteiros de <em>8 bits</em>.
<ul>
<li>Algoritmos de ML que utilizam redes neuronais precisam de computacionar um modelo durante a fase de treino, onde produtos de matrizes intensivos são feitos sem necessitarem de grande precisão.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="unicore-multithreading"><a class="header" href="#unicore-multithreading"><em>Unicore Multithreading</em></a></h2>
<ul>
<li>Corre múltiplas <em>threads</em> numa execução em paralelo.
<ul>
<li>Partilha todos os recursos, exceto registos replicados, PC/IP, etc...</li>
<li>Troca rapidamente entre <em>threads</em>.</li>
</ul>
</li>
<li>Abordagens:
<ol>
<li><em>Fine-grain Multithreading</em>/<em>time-multiplexed multithreading</em>
<ol>
<li>Troca de <em>threads</em> após cada <em>clock cycle</em>;</li>
<li>Intercala a execução de instruções;</li>
<li>Se uma <em>thread</em> parar, as outras são executadas.</li>
</ol>
</li>
<li><em>Coarse-grain Multithreading</em>
<ol>
<li>Só troca de <em>thread</em> em grandes paragens (p.e. uma <em>miss</em> na <em>cache</em> de nível 2);</li>
<li>Simplifica o <em>hardware</em>, mas não esconde pequenas paragens (p.e. para dependências de dados).</li>
</ol>
</li>
<li><em>Simultaneous multithreading</em>
<ol>
<li>O processador agenda os múltiplos problemas de forma dinâmica:
<ol>
<li>Agenda instruções das múltiplas <em>threads</em>;</li>
<li>As instruções de <em>threads</em> independentes são executadas quando as unidades funcionais estiverem disponíveis;</li>
<li>Entre <em>threads</em>, as dependências são geridas através do agendamento e renomeação de registos.</li>
</ol>
</li>
<li>Exemplo: Intel do <em>Pentium-4 HT</em>.
<ol>
<li>Duas <em>threads</em>: registos duplicados, unidade funcional e <em>caches</em> partilhadas. </li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aceleradores-de-computação-gpu-e-cuda"><a class="header" href="#aceleradores-de-computação-gpu-e-cuda">Aceleradores de Computação: GPU e CUDA</a></h1>
<h2 id="aceleradores-de-computação"><a class="header" href="#aceleradores-de-computação">Aceleradores de Computação</a></h2>
<ul>
<li>O melhor acelerador para <em>number crunching</em> ou para computação intensiva de vetores/matrizes é a GPU;</li>
<li>Temos ainda outros aceleradores comuns:
<ul>
<li><strong>DSP</strong>: <em>Digital Signal Processor</em>;
<ul>
<li>Usado, maioritariamente, em equipamentos de telecomunicações.</li>
</ul>
</li>
<li><strong>TPU</strong>: <em>Tensor Unit Processing Units</em>;
<ul>
<li>Otimizado para operações com tensores (vetores e matrizes n-dimensionais), popularizado em aplicações AI, nomeadamente para condução autónoma.</li>
</ul>
</li>
<li><strong>FPGA</strong>: <em>Field Programmable Gate Arrays</em>.
<ul>
<li><em>Hardware</em>/<em>Software</em> reconfigurável;</li>
<li>Pode ser configurado em <em>runtime</em> de forma a comportar-se como uma dada especificação.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="graphical-processing-units-gpu"><a class="header" href="#graphical-processing-units-gpu"><em>Graphical Processing Units</em> (GPU)</a></h2>
<ul>
<li>Ideia base:
<ul>
<li>Modelos de execução heterogéneos;
<ul>
<li>A CPU será o <em>host</em> e a GPU o <em>device</em>.</li>
</ul>
</li>
<li>Deve desenvolver-se um programa numa linguagem semelhante a C para a GPU;</li>
<li>Une todas as formas de paralelismo da GPU como uma <em>thread</em> CUDA;</li>
<li>O modelo de programação segue o SIMT (<em>Single Instruction Multiple Thread</em>).</li>
</ul>
</li>
</ul>
<h3 id="arquitetura-de-uma-gpu-da-nvidia"><a class="header" href="#arquitetura-de-uma-gpu-da-nvidia">Arquitetura de uma GPU da Nvidia</a></h3>
<ul>
<li>Semelhanças com máquinas vetoriais:
<ul>
<li>Trabalha bem com problemas de paralelismo ao nível dos dados;</li>
<li>Aplica o <em>scatter-gather</em> às transferências;</li>
<li>Aplica <em>masks</em> aos registos;</li>
<li>Tem grandes ficheiros de registo.</li>
</ul>
</li>
<li>Diferenças:
<ul>
<li>Não tem um processador escalar;</li>
<li>Utiliza <em>multithreading</em> de forma a esconder a latência da memória;</li>
<li>Tem múltiplas unidades funcionais, mas poucas unidades em <em>pipeline</em>, trabalhando como um processador vetorial.</li>
</ul>
</li>
</ul>
<h3 id="terminologia-1"><a class="header" href="#terminologia-1">Terminologia</a></h3>
<ul>
<li>Cada <em>thread</em> está limitada a 64 registos;</li>
<li>Grupos de 32 <em>threads</em> estão combinados numa <em>thread</em> SIMD, também chamada <em>warp</em>;
<ul>
<li>Mapeada em 16 pistas físicas.</li>
</ul>
</li>
<li>Até 32 <em>warps</em> são agendados num único processador SIMD (SM);
<ul>
<li>Cada <em>warp</em> terá o seu próprio PC;</li>
<li>O agente responsável pelo agendamento das <em>threads</em> utiliza um <em>scoreboard</em> de forma a expedir as <em>threads</em>;</li>
<li>Por definição, não existem dependências de dados entre <em>warps</em>;</li>
<li>Ao expedir os <em>warps</em> para <em>pipelines</em>, esconderá a latência da memória.</li>
</ul>
</li>
<li>Cada processador SIMD (SM):
<ul>
<li>Tem 32 pistas SIMD;</li>
<li>É largo e mais raso quando comparado com processadores vetoriais.</li>
</ul>
</li>
</ul>
<h3 id="estruturas-de-memória-de-uma-gpu-da-nvidia"><a class="header" href="#estruturas-de-memória-de-uma-gpu-da-nvidia">Estruturas de Memória de uma GPU da Nvidia</a></h3>
<ul>
<li>Cada pista SIMD tem uma secção privada de um <em>off-chip</em> DRAM;
<ul>
<li>&quot;Memória Privada&quot; (na terminologia da Nvidia, <em>Local Memory</em>);</li>
<li>Contém um <em>stack frame</em>, <em>spilling registers</em> e variáveis privadas.</li>
</ul>
</li>
<li>Cada processador SIMD <em>multithreaded</em> (na terminologia da Nvidia, <em>SM</em>) também possui memória local (na termionologia da Nvidia, <em>Shared Memory</em>);
<ul>
<li>Partilhada pelas pistas SIMD/<em>threads</em> num dado bloco.</li>
</ul>
</li>
<li>A memória partilhada pelos processadores SIMD é memória da GPU e o <em>off-chip</em> DRAM (na terminologia da Nvidia, <em>Global Memory</em>).
<ul>
<li>O <em>host</em> poderá ler e escrever na memória da GPU.</li>
</ul>
</li>
</ul>
<h3 id="inovações-da-arquitetura-pascal-maio-2016"><a class="header" href="#inovações-da-arquitetura-pascal-maio-2016">Inovações da Arquitetura Pascal (maio 2016)</a></h3>
<ul>
<li>Cada processador SIMD tem:
<ul>
<li>Dois ou 4 agentes de agendamento de <em>threads</em> SIMD, duas unidades de expedição de instruções;</li>
<li>4 pistas de 16 SIMD, 16 unidades de <em>load-store</em> e 16 unidades de funções especiais;</li>
<li>2 <em>threads</em> de instruções SIMD são agendadas a cada 2 <em>clock cycles</em>.</li>
</ul>
</li>
<li>Introdução de:
<ul>
<li><em>Fast single-precision</em>;</li>
<li><em>Doube-precision</em>;</li>
<li><em>Half-precision</em>.</li>
</ul>
</li>
<li><em>High Bandwith Memory 2</em> (HBM2) com 732 GB/s;</li>
<li><em>NVLink</em> entre múltiplas GPUs (20 GB/s em cada direção);</li>
<li>Memória virtual e <em>paging support</em> unificados.</li>
</ul>
<h2 id="arquiteturas-vetoriais-vs-gpus"><a class="header" href="#arquiteturas-vetoriais-vs-gpus">Arquiteturas Vetoriais <em>vs</em> GPUs</a></h2>
<ul>
<li>Processador SIMD análogo ao processador vetorial, ambos têm MIMD;</li>
<li>Registos:
<ul>
<li>O ficheiro de registo RV64V é capaz de manter vetores inteiros, enquanto que a GPU irá distribuir os vetores em registos das pistas SIMD;</li>
<li>O RV64V tem 32 registos de vetores de 32 elementos (total de 1024 elementos), já a GPU tem 256 registos com 32 elementos (total de 8000 elementos);</li>
<li>RV64V tem 2 a 8 pistas com o tamanho do vetor a ser de 32, e o <em>chime</em> terá entre 4 a 16 ciclos, já no proocessador SIMD o <em>chime</em> terá 2 a 4 ciclos;
<ul>
<li><em>quick reminder</em>: o <em>chime</em> é uma unidade de tempo que demora a executar um <em>convoy</em>;
<ul>
<li><em>convoy</em>: um conjunto de instruções vetoriais que, potencialmente, poderá começar a execução em conjunto num período de relógio.</li>
</ul>
</li>
</ul>
</li>
<li>O <em>loop</em> vetorizado da GPU é uma grelha;</li>
<li>Todas as instruções de leitura da GPU são <em>gather</em> e todas as instruções de guardar são <em>scatter</em>.</li>
</ul>
</li>
</ul>
<h2 id="arquiteturas-simd-vs-gpus"><a class="header" href="#arquiteturas-simd-vs-gpus">Arquiteturas SIMD <em>vs</em> GPUs</a></h2>
<ul>
<li>As GPUs possuem mais pistas SIMD;</li>
<li>As GPUs têm <em>hardware</em> para suportar mais <em>threads</em>;</li>
<li>Ambas têm um rácio de 2:1 entre o desempenho entre <em>double-precision</em> e <em>single-precision</em>;</li>
<li>Ambas têm endereços de 64 <em>bits</em>, mas as GPUs têm menor memória;</li>
<li>Arquiteturas SIMD não têm suporte para instruções <em>scatter-gather</em>.</li>
</ul>
<h2 id="modelo-de-programação-cuda"><a class="header" href="#modelo-de-programação-cuda">Modelo de programação CUDA</a></h2>
<ul>
<li><em>Compute Unified Device Architecture</em>;</li>
<li>Desenhado para:
<ul>
<li><em>hosts</em> com CPUs <em>multicore</em> acoplados a dispositivos <em>many-core</em> onde:
<ul>
<li>os dispositivos têm um grande paralelismo SIMD/SIMT;</li>
<li>o <em>host</em> e o dispositivo não partilham memória.</li>
</ul>
</li>
</ul>
</li>
<li>Providencia:
<ul>
<li>uma abstração de <em>threads</em> de forma a lidar com o SIMD;</li>
<li>sincronização e partilha de dados entre pequenos grupos de <em>threads</em>.</li>
</ul>
</li>
<li>Os programas em CUDA são escritos em C com extensões;</li>
<li>O OpenCL é inspirado no CUDA, mas o <em>hardware</em> e o <em>software</em> não têm um produtor específico.
<ul>
<li>O modelo de programação é, essencialmente, idêntico.</li>
</ul>
</li>
</ul>
<h3 id="dispositivos-cuda-e-threads"><a class="header" href="#dispositivos-cuda-e-threads">Dispositivos CUDA e <em>Threads</em></a></h3>
<ul>
<li>Um dispositivo de computação:
<ul>
<li>é um co-processador para a CPU ou <em>host</em>;</li>
<li>tem a sua própria DRAM (chamada, <em>device memory</em>);</li>
<li>corre múltiplas <em>threads</em> em paralelo;</li>
<li>tipicamente, é uma GPU, mas também ser qualquer outro tipo de dispositivo de processamento em paralelo.</li>
</ul>
</li>
<li>As porções de dados paralelos de uma aplicação são expressas com <em>kernels</em> do dispostivo que irão correr em múltiplas <em>threads</em> (SIMT);</li>
<li>Diferenças entre <em>threads</em> de GPU e <em>threads</em> de CPU:
<ul>
<li><em>threads</em> de GPU são extremamente leves;
<ul>
<li>pouco <em>overhead</em> de criação, mas requerem um grande banco de registos.</li>
</ul>
</li>
<li>a GPU precisa de milhares de <em>threads</em> para apresentar a sua eficiência mãxima;
<ul>
<li>por outro lado, os CPUs <em>multicore</em> precisam de poucas.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="modelo-básico-de-cuda-single-program-multiple-data-spmd"><a class="header" href="#modelo-básico-de-cuda-single-program-multiple-data-spmd">Modelo básico de CUDA: <em>Single Program, Multiple Data</em> (SPMD)</a></h3>
<ul>
<li>CUDA integrado na CPU + aplicação de um programa em C na GPU;
<ul>
<li>Código sequencial C executa na CPU;</li>
<li><em>Kernel</em> paralelo C executa em blocos de <em>threads</em> na GPU.</li>
</ul>
</li>
</ul>
<h3 id="spmd--simtsimd"><a class="header" href="#spmd--simtsimd">SPMD + SIMT/SIMD</a></h3>
<ul>
<li><strong>Hierarquia</strong>:
<ul>
<li>Dispostivo \( \rightarrow \) Grelhas;</li>
<li>Grelhas \( \rightarrow \) Blocos;</li>
<li>Blocos \( \rightarrow \) <em>Warps</em>;</li>
<li><em>Warps</em> \( \rightarrow \) <em>Threads</em>.</li>
</ul>
</li>
<li>Um único <em>kernel</em> corre em múltiplos blocos (SPMD);</li>
<li>As <em>threads</em> de um <em>warp</em> são executadas numa forma <em>lock-step</em> denominada <em>single instruction, multiple thread</em> (SIMT);</li>
<li>Instruções singulares são executas em múltiplas <em>threads</em> (SIMD);
<ul>
<li>O tamanho do <em>warp</em> define a granularidade do SIMD (32 <em>threads</em>).</li>
</ul>
</li>
<li>A sincronização dentro de um bloco utiliza a memória partilhada.</li>
</ul>
<h3 id="grelha-computacional-block-ids-e-thread-ids"><a class="header" href="#grelha-computacional-block-ids-e-thread-ids">Grelha Computacional: <em>Block IDs</em> e <em>Thread IDs</em></a></h3>
<ul>
<li>Um <em>kernel</em> corre numa grelha computacional de blocos de <em>threads</em>;
<ul>
<li>As <em>threads</em> utilizam memória global partilhada.</li>
</ul>
</li>
<li>Cada <em>thread</em> utiliza IDs para decidir em que dados deve trabalhar;
<ul>
<li>ID do bloco: 1D ou 2D;</li>
<li>ID da <em>thread</em>: 1D, 2D ou 3D.</li>
</ul>
</li>
<li>Um bloco de <em>threads</em> é um <em>batch</em> de <em>threads</em> que podem cooperar entre si:
<ul>
<li>Sincronizam a sua execução com uma barreira;</li>
<li>Partilham dados de forma eficiente através de memória partilhada de baixa latência;</li>
<li>Duas <em>threads</em> de dois diferentes blocos não podem cooperar entre si.</li>
</ul>
</li>
</ul>
<h3 id="terminologia-e-terminologia-da-nvidia"><a class="header" href="#terminologia-e-terminologia-da-nvidia">Terminologia (e terminologia da Nvidia)</a></h3>
<ul>
<li><em>Threads</em> de instruções SIMD (<em>warps</em>);
<ul>
<li>Cada uma tem o seu próprio <em>Instruction Pointer</em> (até 48/64 por processador SIMD);</li>
<li>O agente responsável por agendar <em>threads</em> utiliza <em>scoreboards</em> para as expedir;</li>
<li>Não existem dependências de dados entre <em>threads</em>;</li>
<li>As <em>threads</em> são organizadas em blocos e executadas em grupos de 32 <em>threads</em> (bloco de <em>threads</em>).
<ul>
<li>Os blocos estão organizados numa grelha.</li>
</ul>
</li>
</ul>
</li>
<li>O agente responsável por agendar os <em>thread blocks</em> agenda-os para os processadores SIMD (<em>Streaming Multiprocessors</em>);</li>
<li>Em cada processador SIMD:
<ul>
<li>32 pistas SIMD (<em>thread processors</em>);</li>
<li>Mais largos e rasos quando comparados a processadores vetoriais.</li>
</ul>
</li>
</ul>
<h3 id="bloco-de-threads-cuda"><a class="header" href="#bloco-de-threads-cuda">Bloco de <em>Threads</em> CUDA</a></h3>
<ul>
<li>O programador declara o bloco (de <em>threads</em>):
<ul>
<li>tamanho irá variar entre 1 a 512 <em>threads</em> concorrentes;</li>
<li>terá um formato 1D, 2D ou 3D;</li>
<li>dimensões dos blocos em <em>threads</em>.</li>
</ul>
</li>
<li>Todos as <em>threads</em> num bloco executam o mesmo programa de <em>thread</em>;</li>
<li>As <em>threads</em> partilham os dados e sincronizam-se enquanto partilham o seu trabalho;</li>
<li>As <em>threads</em> têm números de identificação dentro de um bloco;</li>
<li>Os programas de <em>threads</em> utilizam os <em>thread ID</em> de forma a selecionar os trabalhos e endereçar os dados partilhados.</li>
</ul>
<pre><code class="language-c">float x = input[threadID];
float y = func(x);
output[threadID] = y;
</code></pre>
<h3 id="partilha-de-memória-partilhada"><a class="header" href="#partilha-de-memória-partilhada">Partilha de memória partilhada</a></h3>
<ul>
<li>Memória Local (por <em>thread</em>):
<ul>
<li>Privada por <em>thread</em>;</li>
<li>Variáveis automáticas, <em>register spill</em>.
<ul>
<li><em>quick reminder</em>: ocorre <em>register spill</em> quando os registos da CPU estão cheios, pelo que o conteúdo tem de ser, temporariamente, guardado em memória.</li>
</ul>
</li>
</ul>
</li>
<li>Memória Partilhada (por bloco):
<ul>
<li>Partilhada por <em>threads</em> no mesmo bloco;</li>
<li>Comunicação inter-<em>threads</em>.</li>
</ul>
</li>
<li>Memória Global (por aplicação):
<ul>
<li>Partilhada por todas as <em>threads</em>;</li>
<li>Comunicação inter-grelhas.</li>
</ul>
</li>
</ul>
<h3 id="overview-do-modelo-de-memória-cuda"><a class="header" href="#overview-do-modelo-de-memória-cuda"><em>Overview</em> do modelo de memória CUDA</a></h3>
<ul>
<li>Cada <em>thread</em> pode:
<ul>
<li>Ler/Escrever <em>per-thread</em> em registos;</li>
<li>Ler/Escrever <em>per-thread</em> em memória local;</li>
<li>Ler/Escrever <em>per-block</em> em memória partilhada;</li>
<li>Ler/Escrever <em>per-grid</em> em memória global;</li>
<li>Ler apenas <em>per-grid</em> em memória constante;</li>
<li>Ler apenas <em>per-grid</em> em memória de textura.</li>
</ul>
</li>
<li>O <em>host</em> pode ler e escrever em memória global, constante ou de textura.</li>
</ul>
<h3 id="implementação-em-hardware-arquitetura-de-memória"><a class="header" href="#implementação-em-hardware-arquitetura-de-memória">Implementação em <em>Hardware</em>: Arquitetura de Memória</a></h3>
<ul>
<li>Memória do Dispositivo (DRAM):
<ul>
<li>Lenta (2 a 300 ciclos);</li>
<li>Em memória local, global, constante ou de textura.</li>
</ul>
</li>
<li><em>On-chip memory</em>:
<ul>
<li>Rápida (1 ciclo);</li>
<li>Registos, memória partilha e <em>cache</em> de constantes/texturas.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introdução-à-programação-com-memória-partilhada"><a class="header" href="#introdução-à-programação-com-memória-partilhada">Introdução à Programação com Memória Partilhada</a></h1>
<h2 id="níveis-de-paralelismo-hardware-e-software"><a class="header" href="#níveis-de-paralelismo-hardware-e-software">Níveis de Paralelismo (<em>Hardware</em> e <em>Software</em>)</a></h2>
<ul>
<li>Instrução (ILP):
<ul>
<li>Execução de múltiplas instruções de um programa em paralelo;</li>
<li>Processamento vetorial;</li>
<li>Explorado pelo <em>hardware</em> atual;</li>
<li>Limitado pelas dependências de dados/controlo do programa.</li>
</ul>
</li>
<li>Tarefas/Fios de Execução
<ul>
<li>Múltiplos fluxos de instruções de um mesmo programa executam em paralelo;</li>
<li>Limitado pelas dependências e características do algoritmo.</li>
</ul>
</li>
<li>Processos
<ul>
<li>Múltiplos processos de um mesmo programa/vários programas.</li>
</ul>
</li>
</ul>
<h2 id="desenvolvimento-de-aplicações-paralelas"><a class="header" href="#desenvolvimento-de-aplicações-paralelas">Desenvolvimento de Aplicações Paralelas</a></h2>
<p><strong>Partição do problema e dos dados a processar</strong>:</p>
<ul>
<li>Identifica oportunidades de paralelismo:
<ul>
<li>Define um elevado número de tarefas (de grão fino);</li>
<li>Pode obter várias decomposições alternativas.</li>
</ul>
</li>
<li>Duas vertentes complementares na identificação das tarefas:
<ul>
<li><strong>Decomposição dos Dados</strong>: identifica dados que podem ser processados em paralelo;
<ul>
<li>foca-se nos dados e processar e na sua divisão em conjuntos que possam ser processados em paralelo.</li>
</ul>
</li>
<li><strong>Decomposição Funcional</strong>: identifica fases do algoritmo que podem ser efetuadas em paralelo.
<ul>
<li>foca-se no processamento a realizar, dividindo-o em tarefas independentes.</li>
</ul>
</li>
</ul>
</li>
<li>A partição deverá obter um número de tarefas, pelo menos, uma ordem de magnitude superior ao número de unidades de processamento.
<ul>
<li>Introduz flexibilidade nas fases posteriores do desenvolvimento.</li>
</ul>
</li>
<li>Tarefas de dimensões idênticas facilitam a distribuição da carga;</li>
<li>O número de tarefas deve aumentar em conformidade com a dimensão do problema.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="programação-paralela-com-memória-partilhada"><a class="header" href="#programação-paralela-com-memória-partilhada">Programação Paralela com Memória Partilhada</a></h1>
<h2 id="especificação-de-concorrênciaparalelismo"><a class="header" href="#especificação-de-concorrênciaparalelismo">Especificação de Concorrência/Paralelismo</a></h2>
<ul>
<li><strong>Processos</strong>
<ul>
<li>Usados para tarefas não correlacionadas;
<ul>
<li>Por exemplo, um programa.</li>
</ul>
</li>
<li>Contém espaço de endereçamento;
<ul>
<li>Que é protegido de outros processos.</li>
</ul>
</li>
<li>Efetua <em>switching</em> ao nível do <em>kernel</em>;
<ul>
<li>Troca entre diferentes processos ou tarefas que estão a ser geridas pelo <em>kernel</em>.</li>
</ul>
</li>
<li>Cada processo tem, pelo menos, uma <em>thread</em>.</li>
</ul>
</li>
<li><em><strong>Threads</strong></em>
<ul>
<li>São parte do mesmo trabalho;</li>
<li>Partilham espaço de endereçamento, código, dados e ficheiros;</li>
<li>Efetuam <em>switching</em> ao nível do utilizador e do <em>kernel</em>.</li>
</ul>
</li>
</ul>
<h2 id="processosthreads-vs-tasks"><a class="header" href="#processosthreads-vs-tasks">Processos/<em>Threads</em> vs <em>Tasks</em></a></h2>
<ul>
<li><em>Task</em>: sequência de instruções;
<ul>
<li>Em Java: <em>Runnable Object</em>;</li>
</ul>
</li>
<li><em>Thread</em>/Processo: execução de uma <em>task</em> num dado contexto;
<ul>
<li>Em Java: <em>Thread</em>.</li>
</ul>
</li>
<li>Processador/<em>core</em>: <em>hardware</em> que corre uma <em>thread</em>/processo.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="medição-e-otimização-de-desempenho-em-memória-partilhada"><a class="header" href="#medição-e-otimização-de-desempenho-em-memória-partilhada">Medição e Otimização de Desempenho em Memória Partilhada</a></h1>
<h2 id="desempenho-em-aplicações-paralelas"><a class="header" href="#desempenho-em-aplicações-paralelas">Desempenho em Aplicações Paralelas</a></h2>
<ul>
<li>Qual é a definição de desempenho?
<ul>
<li>Existem múltiplas alternativas.</li>
<li>Tempo de execução, escalabilidade, requisitos de memória, latência, débito, custos, portabilidade, potencial de reutilização, etc...</li>
<li>A importância de cada um, dependerá da aplicação em concreto.</li>
</ul>
</li>
<li>A medida mais comum das aplicações paralelas é o tempo de execução ou <em>speed-up</em>.
<ul>
<li>Temos da melhor implementação sequencial <em>vs</em> Tempo da versão paralela;</li>
<li>Forte análise da escalabilidade:
<ul>
<li>aumento do <em>speed-up</em> com a PU para um problema com um tamanho fixo de dados;</li>
<li>O <em>speed-up</em> ideal é proporcional à PU.</li>
</ul>
</li>
<li>Fraca análise da escalabilidade:
<ul>
<li>Aumentam o problema do tamanho de dados assim que o número de PUs aumenta;</li>
<li>Idealmente, o tempo de execução dever-se-ia manter constante.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="lei-de-amdhal-forte-análise-da-escalabilidade"><a class="header" href="#lei-de-amdhal-forte-análise-da-escalabilidade">Lei de Amdhal (Forte Análise da Escalabilidade)</a></h3>
<ul>
<li>Mede o tempo da versão paralela (\( T_{par} \)) assim que o número de PUs aumenta;</li>
<li>O \( T_{seq} \) pode ser dividida em:
<ul>
<li>Tempo a fazer trabalho não paralelizável (trabalho sequencial);</li>
<li>Tempo a fazer trabalho paralelizável.</li>
</ul>
</li>
<li>A fração do trabalho não paralelizável irá limitar o <em>speed-up</em> máximo;
<ul>
<li>\( P \) \( \rightarrow \) número de PUs;</li>
<li>\( f \) \( \rightarrow \) fração sequencial.</li>
</ul>
</li>
</ul>
<p>\[ S_P \leq \frac{1}{f + (1 - f) / P} \]
\[ Max_{speedup} = \frac{1}{serial\ fraction\ of\ work} \]</p>
<ul>
<li>Isto reforça a ideia que devemos preferir algoritmos que suportem uma execução paralela: <em>think parallel</em>.</li>
</ul>
<h3 id="anomalias-de-speed-up"><a class="header" href="#anomalias-de-speed-up">Anomalias de <em>Speed-up</em></a></h3>
<ul>
<li><em>Super-linear</em> (o ganho é maior que o número de PUs): em muitos casos isto deve-se aos efeitos da <em>cache</em>.</li>
</ul>
<h3 id="lei-de-gustafson-fraca-análise-da-escalabilidade"><a class="header" href="#lei-de-gustafson-fraca-análise-da-escalabilidade">Lei de Gustafson (Fraca Análise da Escalabilidade)</a></h3>
<ul>
<li>Aumenta o tamanho do problema consoante o número de PUs aumenta;
<ul>
<li>Grandes recursos computacionais, geralmente, são utilizados para problemas de grandes dimensões.</li>
</ul>
</li>
<li>A fração de trabalho sequencial, geralmente, diminui com o tamanho do problema.</li>
</ul>
<h3 id="estudos-experimentais"><a class="header" href="#estudos-experimentais">Estudos Experimentais</a></h3>
<ul>
<li>Perfil de execução sequencial:
<ul>
<li>Identifica <em>hot-spots</em> de aplicações:
<ul>
<li>Funções que levam mais tempo a executar.</li>
</ul>
</li>
<li>Podem ser implementadas por ferramentas específicas ou instrumentalizar diretamente o código:
<ul>
<li>Existe sempre <em>overhead</em> introduzido na base de aplicação.</li>
</ul>
</li>
</ul>
</li>
<li>Perfil de execução paralela:
<ul>
<li>Recolha dados de desempenho <em>per-thread</em>;</li>
<li>Mais difícil de interpretar.</li>
</ul>
</li>
<li>Os <em>hot-spots</em> podem alterar enquanto a aplicação é melhorada.</li>
</ul>
<h3 id="técnicas-para-fazer-profiling"><a class="header" href="#técnicas-para-fazer-profiling">Técnicas para fazer <em>profiling</em></a></h3>
<ul>
<li><em>Polling (sampling)</em>
<ul>
<li>A aplicação é, periodicamente, interrompida para colecionar dados de desempenho;</li>
<li>Exemplos: <code>gprof</code> e <code>perf record</code>.</li>
</ul>
</li>
<li><em>Instrumentation</em>
<ul>
<li>O código é introduzido (pelo programador ou ferramentas) para colecionar dados de desempenho acerca de eventos úteis:
<ul>
<li>Tende a produzir melhores resultados, mas mais <em>overhead</em>.</li>
</ul>
</li>
<li>Exemplos: <code>Valgrind</code>.</li>
</ul>
</li>
</ul>
<h2 id="problemas-de-escalabilidade-na-memória-partilhada"><a class="header" href="#problemas-de-escalabilidade-na-memória-partilhada">Problemas de Escalabilidade na Memória Partilhada</a></h2>
<h3 id="algumas-razões-para-as-aplicações-não-terem-o-speed-up-ideal"><a class="header" href="#algumas-razões-para-as-aplicações-não-terem-o-speed-up-ideal">Algumas Razões para as Aplicações não terem o Speed-Up ideal?</a></h3>
<ul>
<li>Trabalho Sequencial:
<ul>
<li>% de trabalho sequencial (Lei de Amdahl);</li>
<li>Barreira de Memória.
<ul>
<li>Serializa os acessos à memória.</li>
</ul>
</li>
</ul>
</li>
<li><em>Overhead</em> de Paralelismo:
<ul>
<li>Granularidade de Paralelismo/<em>Task</em>.
<ul>
<li>Trabalho adicional feito em aplicações paralelas (gestão de tarefas, computações redundantes, etc...).</li>
</ul>
</li>
</ul>
</li>
<li><em>Overhead</em> de Paralelismo <strong>e</strong> Trabalho Sequencial/Tempo em <em>Idle</em>:
<ul>
<li><em>Overhead</em> de Sincronização.
<ul>
<li>Também deveria serializar a execução (p.e. <em>critical</em>).
<ul>
<li>Inclui múltiplas chamadas a rotinas externas (p.e. <code>malloc</code>).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Trabalho Sequencial/Tempo em <em>Idle</em>
<ul>
<li><em>Over-decomposition</em> poderá melhorar o <em>load balancing</em>.</li>
</ul>
</li>
</ul>
<h3 id="razões-para-a-falta-da-escalabilidade"><a class="header" href="#razões-para-a-falta-da-escalabilidade">Razões para a Falta da Escalabilidade</a></h3>
<ul>
<li>Limitação à largura de banda na memória/<em>cache</em>:
<ul>
<li>Diagnóstico (algumas opções):
<ul>
<li>Medir a largura de banda (de memória, por <em>core</em>) necessária e comparar à largura de banda necessária;</li>
<li>Utilização do <em>roofline model</em> para analisar a intensidade computacional;</li>
<li>O CPI aumenta com o número de <em>threads</em>.</li>
</ul>
</li>
<li>Ações:
<ul>
<li>Melhorar a localidade de dados.</li>
</ul>
</li>
<li>Abordagem:
<ul>
<li>Converter <em>Arrays of Points</em> em <em>Arrays of Structures</em> ou <em>Structures of Arrays</em>.</li>
</ul>
</li>
</ul>
</li>
<li>Paralelismo de grão fino (demasiado <em>overhead</em> no paralelismo):
<ul>
<li>Diagnóstico:
<ul>
<li>Medir a granularidade das tarefas.</li>
</ul>
</li>
<li>Ação:
<ul>
<li>Aumentar a granularidade das tarefas para diminuir o <em>overhead</em> do paralelismo.</li>
</ul>
</li>
<li>Abordagens:
<ul>
<li>Favorecer <em>static loop schedulling</em> (em certos casos precisa de ser implementado explicitamente);</li>
<li>Diminuir a frequência de criação de tarefas.</li>
</ul>
</li>
</ul>
</li>
<li>Demasiada sincronização de tarefas (devido a dependências):
<ul>
<li>Diagnóstico:
<ul>
<li>Correr tarefas sem sincronizá-las (irão produzir resultados errados).</li>
</ul>
</li>
<li>Ação:
<ul>
<li>Remover sincronização.</li>
</ul>
</li>
<li>Abordagens:
<ul>
<li>Aumenta a granularidade das tarefas;</li>
<li>Computações redundates/especulativas;</li>
<li>Utilização de valores de <em>threads</em> locais (preciso ter cuidado com falsa partilha de linhas de <em>cache</em>/utilização de memória).</li>
</ul>
</li>
</ul>
</li>
<li>Má distribuição de carga:
<ul>
<li>Diagnóstico:
<ul>
<li>Medir o tempo de cada tarefa computacional.</li>
</ul>
</li>
<li>Ação:
<ul>
<li>Melhorar o agendamento/mapeamento.</li>
</ul>
</li>
<li>Abordagens:
<ul>
<li>Agendamento cíclico/dinâmico/guiado;</li>
<li>Agendamento (estático) personalizado do <em>loop</em>.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="medir-o-desempenho"><a class="header" href="#medir-o-desempenho">Medir o desempenho</a></h2>
<h3 id="princípios"><a class="header" href="#princípios">Princípios</a></h3>
<ul>
<li>Isolar de fatores externos;
<ul>
<li>Considerar o <em>overhead</em> inserido pela medição;</li>
<li>Repetir várias vezes a medição;</li>
<li>Evitar sobrecargas do sitema.</li>
</ul>
</li>
<li>Documentar o ocorrido para que pode ser replicado por outros;
<ul>
<li><em>Hardware</em>, versões de <em>software</em>, estado do sistema, etc...</li>
</ul>
</li>
<li><strong>Importante</strong>: Resolução do Relógio
<ul>
<li>Precisão: diferença entre o tempo medido e o tempo real;</li>
<li>Resolução: unidade de tempo entre os incrementos do relógio.
<ul>
<li>Em princípio, não é possível medir eventos menores que a resolução do relógio, mas...</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="tempo-para-correr-uma-aplicação"><a class="header" href="#tempo-para-correr-uma-aplicação">Tempo para correr uma aplicação</a></h3>
<ul>
<li><strong>Tempo de CPU</strong>
<ul>
<li>Tempo dedicado, em exclusivo, para a execução do programa;</li>
<li>Não depende de outras atividades do sistema.</li>
</ul>
</li>
<li><em><strong>Wall Time</strong></em>
<ul>
<li>Tempo medido entre o início e o fim da execução;</li>
<li>Dependerá da carga do sistema, de I/O, etc...</li>
</ul>
</li>
<li><strong>Complexidades</strong>
<ul>
<li>Agendamento de processos (10ms?);</li>
<li>Carga introduzida por outros processos (p.e. pelo <em>garbage collector</em> em Java).</li>
</ul>
</li>
</ul>
<h3 id="opções-para-medir-o-tempo"><a class="header" href="#opções-para-medir-o-tempo">Opções para medir o tempo</a></h3>
<ul>
<li><code>time</code> na <em>command line</em>;
<ul>
<li>Apenas para medidas superiores a 1s.</li>
</ul>
</li>
<li><code>gettimeofday()</code>
<ul>
<li>Devolve o número de microsegundos desde o dia 1 de janeiro de 1970;</li>
<li>Utiliza o &quot;<em>Timer</em>&quot; ou o contador de ciclos (depende da plataforma);</li>
<li>No melhor caso: 1 microsegundo.</li>
</ul>
</li>
<li>Contador de <em>clock cycle</em> (introduzido nos processadores modernos)
<ul>
<li>Grande resolução;</li>
<li>Útil para medidas inferiores a 1s.</li>
</ul>
</li>
<li>Por fim, temos as abordagens preferidas que são implementações de alta resolução:
<ul>
<li>Função de <em>timer</em> do <em>OpenMP</em>/MPI
<ul>
<li><code>omp_get_wtime</code>, <code>omp_get_wtick</code>;</li>
<li><code>MPI_WTime</code>.</li>
</ul>
</li>
<li><code>System.nanoTime()</code> em Java 4.</li>
</ul>
</li>
</ul>
<h3 id="como-combinar-os-resultados-das-várias-medições"><a class="header" href="#como-combinar-os-resultados-das-várias-medições">Como combinar os resultados das várias medições?</a></h3>
<ul>
<li>Média;
<ul>
<li>Afetada por valores extremamente altos/baixos;</li>
<li>Adicionalmente, podemos mostrar o desvio padrão entre as diversas medidas.</li>
</ul>
</li>
<li>Melhor medida;
<ul>
<li>Valor nas condições ideias.</li>
</ul>
</li>
<li>Média dos k-melhores;
<ul>
<li>Remove os <em>outsiders</em>.</li>
</ul>
</li>
<li>Mediana.
<ul>
<li>Mais robusta para grandes variações.</li>
</ul>
</li>
</ul>
<h3 id="apresentação-de-resultados"><a class="header" href="#apresentação-de-resultados">Apresentação de Resultados</a></h3>
<ul>
<li>Apresentação de formas legíveis e compactas;
<ul>
<li>Tabelas ou Gráficos.</li>
</ul>
</li>
<li>Devem colocar-se legendas claras nas tabelas e gráficos;</li>
<li>Não se devem extrapolar valores;
<ul>
<li>Utilizar sempre o número de dígitos significantes.</li>
</ul>
</li>
<li>Usar incrementos constantes no eixo do \( xx \) e do \( yy \);
<ul>
<li>Escalas podem levar a conclusões erradas;</li>
<li>Representar o 1 ou o 0.</li>
</ul>
</li>
<li>Justificar os resultados obtidos.
<ul>
<li>Investigar/comentar valores inesperados.</li>
</ul>
</li>
</ul>
<h3 id="erros-comuns"><a class="header" href="#erros-comuns">Erros comuns</a></h3>
<ul>
<li>Não documentar o ambiente experimental ou incluír detalhes irrelevantes;</li>
<li>Não repetir as experiências;
<ul>
<li>Reduz o impacto do SO, do <em>garbage collector</em>, etc...</li>
</ul>
</li>
<li>Inclusão do tempo de I/O;
<ul>
<li>Leituras de disco;</li>
<li><code>printf</code> (para, por exemplo, apresentar informação de <em>debug</em>).</li>
</ul>
</li>
<li>Não considerar o <em>overhead</em> de leitura/resolução;</li>
<li>Não aquecer a <em>cache</em>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="programação-em-memória-distribuída-com-passagem-de-mensagens"><a class="header" href="#programação-em-memória-distribuída-com-passagem-de-mensagens">Programação em Memória Distribuída com Passagem de Mensagens</a></h1>
<h2 id="passagem-de-mensagens"><a class="header" href="#passagem-de-mensagens">Passagem de Mensagens</a></h2>
<h3 id="conceitos-básicos"><a class="header" href="#conceitos-básicos">Conceitos Básicos</a></h3>
<ul>
<li>A especificação de atividades paralelas é feita através de processos com espaços de endereçamento disjuntos;
<ul>
<li>Não existe memória partilhada entre os processos, pelo que tem de existir uma passagem de mensagens em paralelismo;</li>
<li>Os processos podem ser idênticos (SPMD, p.e. MPI) ou não (MIMD, p.e. PVM).</li>
</ul>
</li>
<li>As atividades paralelas comunicam através de portas ou canais;
<ul>
<li>O envio e a receção da mensagem são explícitos (de/para uma porta ou canal).</li>
</ul>
</li>
<li>Os dados devem ser explicitamente ordenados em mensagens;</li>
<li>Existem primitivas de comunicação mais sofisticadas (<em>broadcast</em>, <em>reduction</em>, <em>barrier</em>).</li>
</ul>
<h3 id="mpi-message-passing-interface"><a class="header" href="#mpi-message-passing-interface">MPI: <em>Message Passing Interface</em></a></h3>
<ul>
<li><em>Standard</em> para a passagem de mensagens;
<ul>
<li>Proveniente de um esforço para desenvolver aplicações paralelas portáteis (baseadas em memória distribuída).</li>
</ul>
</li>
<li>Baseado no modelo SPMD, ou seja, o mesmo código é executado em todos os processos;</li>
<li>Passagem de mensagens com a entrega de mensagens <em>in order</em> utilizando uma comunicação ponto-a-ponto;</li>
<li>Implementada como uma biblioteca de funções;</li>
<li>Bibliotecas Comuns (<em>Open Source</em>): OpenMPI, MPICH e LamMPI;</li>
<li>Principais <em>Features</em>:
<ul>
<li>Diversos modos para passagem de mensagens: síncronos e assíncronos;</li>
<li>Grupos/topologias de comunicação;</li>
<li>Conjunto largo de operações coletivas: <em>Broadcast</em>, <em>Scatter/Gather</em>, <em>Reduce</em>, <em>All-to-all</em>, <em>Barrier</em>;</li>
<li>MPI-2: processos dinâmicos, I/O paralelo, acesso remoto a memória (RMA - <em>put/get</em>);
<ul>
<li>Programação em memória partilhada limitada.</li>
</ul>
</li>
<li>MPI-3: programação em memória explicitamente partilhada.</li>
</ul>
</li>
</ul>
<h3 id="smpd-single-program-multiple-data-model"><a class="header" href="#smpd-single-program-multiple-data-model">SMPD: <em>Single Program Multiple Data model</em></a></h3>
<ul>
<li>O mesmo executável é lançado num conjunto de processos (isto é, em diversas máquinas);
<ul>
<li>Execução assíncrona do mesmo programa;</li>
<li>Cada processo tem um único identificador.</li>
</ul>
</li>
<li>O <em>rank</em> de cada processo é utilizado para definir o comportamento específico de cada processo.
<ul>
<li><em>Process-specific control flow</em>;
<ul>
<li>Processamento de dados e comunicação inter-processos.</li>
</ul>
</li>
<li>Veja-se o seguinte exemplo com 3 processos:</li>
</ul>
</li>
</ul>
<p><img src="images/spmd_example.png" alt="image Exemplo SPMD" /></p>
<ul>
<li>É fácil de escrever um programa que funcione com um número arbitrário de processos (máquinas).</li>
</ul>
<h3 id="estrutura-de-um-programa-mpi"><a class="header" href="#estrutura-de-um-programa-mpi">Estrutura de um programa MPI</a></h3>
<p><img src="images/struct_mpi_prog.png" alt="image Estrutura de um programa MPI" /></p>
<ul>
<li>Para compilar e executar:
<ul>
<li><strong>Compilar</strong>: <code>mpicc</code> (<code>mpicxx</code> para C++);</li>
<li><strong>Executar</strong>: <code>mpirun -np &lt;num de processos&gt; a.out</code></li>
</ul>
</li>
</ul>
<h3 id="mais-funcionalidades-do-mpi"><a class="header" href="#mais-funcionalidades-do-mpi">Mais Funcionalidades do MPI</a></h3>
<ul>
<li>Comunicação ponto-a-ponto entre processos;
<ul>
<li><code>int MPI_Send(void* buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)</code>;</li>
<li><code>int MPI_Recv(void* buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)</code>;</li>
<li>Conteúdo da mensagem de dados: <code>void* buf, int count, MPI_Datatype datatype</code>;
<ul>
<li>Precisa que seja especificado o tipo de dados (<code>MPI_INT</code>, <code>MPI_DOUBLE</code>, etc...).</li>
</ul>
</li>
<li>Cada processo é identificado pelo seu <em>rank</em> no grupo;
<ul>
<li><code>dest/source</code> providenciam o destino e a fonte da mensagem;</li>
<li>Por <em>default</em> existe um grupo composto por todos os processos: <code>MPI_COMM_WORLD</code>;</li>
<li>A <em>tag</em> pode ser utilizada para distinguir diversas mensagens;</li>
<li>O <code>MPI_Recv</code> espera pela chegada de uma mensagem com as características especificadas.
<ul>
<li>Para identificarmos qualquer fonte ou qualquer tag, utiliza-se, respetivamente, <code>MPI_ANY_SOURCE</code> ou <code>MPI_ANY_TAG</code>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="mpi---modos-da-comunicação-ponto-a-ponto"><a class="header" href="#mpi---modos-da-comunicação-ponto-a-ponto">MPI - Modos da comunicação ponto-a-ponto</a></h3>
<ul>
<li><strong><em>Overhead</em> de passagem da mensagem</strong>
<ul>
<li>Tempo de transferência da mensagem (copia-a para a rede, transmite-a na rede e entrega-a no <em>buffer</em> do recetor);</li>
</ul>
</li>
<li>O <em>standard</em> do <code>MPI_Send</code> pode ser implementado de diversas formas;
<ul>
<li>Não irá retornar até se poder usar o <em>buffer</em> de envio, assim, poderá ou não bloquear.</li>
</ul>
</li>
<li>Implementações explícitas para o envio (opções diferentes para <em>buffering</em> e sincronização):
<ul>
<li><code>MPI_Ssend (blocking synchronous send)</code>
<ul>
<li>O emissor aguarda até à mensagem ser recebida;</li>
</ul>
</li>
<li><code>MPI_RSend (ready send)</code>
<ul>
<li>Retorna assim que a mensagem for colocada na rede;</li>
<li>O lado do recetor deve ter um <code>MPI_Recv</code> de forma a evitar <em>deadlocks</em>.</li>
</ul>
</li>
<li><code>MPI_BSend (buffered send)</code>
<ul>
<li>Retorna assim que a mensagem for colocada no <em>buffer</em> do lado do emissor;</li>
<li>Não sofre do <em>overhead</em> da sincronização com o recetor, mas poderá fazer a cópia apenas para um <em>buffer</em> local.</li>
</ul>
</li>
<li><code>MPI_lxxx (non-blocking sends)</code> com <code>MPI_wait</code>, <code>MPI_Test</code>, <code>MPI_Probe</code>.
<ul>
<li>Retorna imediatamente, sendo que o programador é que está responsável por verificar se a operação foi completada (utilizando o <em>wait</em>).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="mpi---comunicações-coletivas"><a class="header" href="#mpi---comunicações-coletivas">MPI - Comunicações Coletivas</a></h3>
<ul>
<li><code>int MPI_Barrier(MPI_Comm comm)</code>;
<ul>
<li>Aguarda até todos os processos terem chegado à <em>Barrier</em>.</li>
</ul>
</li>
<li><code>int MPI_BCast(void* buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)</code>
<ul>
<li>Envia os dados em <em>broadcast</em> desde a raíz até todos os outros processos.</li>
</ul>
</li>
<li><code>int MPI_Gather &amp; int MPI_Scatter(void* sbuf, int scount, MPI_Datatype stype, void* rbuf, int rcount, MPI_Datatype rtype, int root, MPI_Comm comm)</code>
<ul>
<li><em><strong>Gather</strong></em>: Junta os dados de todos os processos na raíz;</li>
<li><em><strong>Scather</strong></em>: Separa os dados da raíz para todos os outros processos.</li>
</ul>
</li>
<li><code>int MPI_Reduce(void* sbuf, void* rbuf, int count, MPI_Datatype stype, MPI_Op op, int root, MPI_Comm comm)</code>
<ul>
<li>Combina os resultados de todos os processos na raíz utilizando a operação especificada em <code>op</code>.</li>
</ul>
</li>
<li>Composições: <em>Allgather</em>, <em>Alltoall</em>, <em>Allreduce</em>, <em>Reduce_scatter</em>.</li>
</ul>
<h3 id="mpi---grupos"><a class="header" href="#mpi---grupos">MPI - Grupos</a></h3>
<ul>
<li>Grupos de processos ordenados;
<ul>
<li>Cada processo tem um <em>rank</em> no grupo;</li>
</ul>
</li>
<li>Escopo para comincação coletiva e ponto-a-ponto</li>
</ul>
<h3 id="mpi---topologias"><a class="header" href="#mpi---topologias">MPI - Topologias</a></h3>
<ul>
<li>Estruturas de processos bem definidas;
<ul>
<li>Cada processo tem um conjunto de vizinhos;
<ul>
<li>Fácil de identificar com uma topologia.</li>
</ul>
</li>
<li>Comunicação através de canais.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="algoritmos-paralelos-sorting"><a class="header" href="#algoritmos-paralelos-sorting">Algoritmos Paralelos (<em>Sorting</em>)</a></h1>
<h2 id="algoritmos-paralelos"><a class="header" href="#algoritmos-paralelos">Algoritmos paralelos</a></h2>
<ul>
<li>Análise Tradicional dos Algoritmos: Notação <em>Big-O</em>
<ul>
<li>Analise o número de operações de um pedaço de código;
<ul>
<li>Por exemplo, ordenação de 100 valores:
<ul>
<li>Mau algoritmo (<em>brute force</em>): \( O(n^2) \);</li>
<li>Melhor: \( O([k]n) \).</li>
</ul>
</li>
<li>Ou inserção numa estrutura de dados:
<ul>
<li>Lista ligada ordenada: \( O(n^2) \);</li>
<li>Árvore binária: \( O(n\ log_2(n)) \);</li>
<li><em>Hash Table</em>: \( O(n) \).</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Intensidade aritmética (operações por movimentações de dados):</li>
</ul>
<p><img src="images/arithmetic_int.png" alt="image Intensidade Aritmética" /></p>
<h2 id="algoritmos-paralelos-de-sorting"><a class="header" href="#algoritmos-paralelos-de-sorting">Algoritmos Paralelos de <em>Sorting</em></a></h2>
<p><img src="images/parallel_sort.png" alt="image Algoritmos Paralelos de Sorting" /></p>
<p><img src="images/method_par_sort.png" alt="image Método" /></p>
<h3 id="em-memória-distribuída"><a class="header" href="#em-memória-distribuída">Em memória distribuída:</a></h3>
<ul>
<li>Problemas de <em>design</em>:
<ul>
<li>Inicialmente, as chaves são distribuídas pelos processadores;
<ul>
<li>É um estágio intermédio para alguns algoritmos paralelos.</li>
</ul>
</li>
<li>Propriedades dos dados;
<ul>
<li>Parcialmente ordenados.</li>
</ul>
</li>
<li>Paralelismo Explorável;
<ul>
<li><em>Merger-based</em>;</li>
<li><em>Splitter-based</em>.</li>
</ul>
</li>
<li>Considerações de eficiência para paralelismo:
<ul>
<li>Movimentos de dados entre processadores;</li>
<li>Balanceamento de carga;</li>
<li>Evitar o tempo em <em>idle</em> (isto é, fases sequenciais).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="merge-sort-em-paralelo"><a class="header" href="#merge-sort-em-paralelo"><em>Merge-Sort</em> em Paralelo</a></h3>
<ul>
<li>Localmente, ordena cada conjunto de dados;</li>
<li>Efetua trocas de conjuntos entre os processadores;</li>
<li>Apenas é efetivo quando \( \frac{n}{p} \approx l \);</li>
<li>Muitos movimentos de dados quando \( \frac{n}{p} \gt \gt l \)</li>
</ul>
<h3 id="quicksort-em-paralelo-simplificado"><a class="header" href="#quicksort-em-paralelo-simplificado"><em>Quicksort</em> em Paralelo (Simplificado)</a></h3>
<ul>
<li>O <em>master</em> seleciona e faz <em>broadcast</em> da chave pivô;</li>
<li>Cada processo, localmente, efetua a separação com base no pivô;
<ul>
<li>Cada processo irá conter uma partição <em>smaller</em> e uma partição <em>greater</em>.</li>
</ul>
</li>
<li>Divide os processadores em conjuntos de <em>smaller</em> e <em>greater</em>;
<ul>
<li>Envia dados para um processador acerca de outro conjunto.</li>
</ul>
</li>
<li>Repete o processo até que o número de conjuntos seja igual ao número de processadores;</li>
<li>Localmente, ordena cada processo p.</li>
</ul>
<p><strong>Complexidade</strong>: precisa de \( log(p) \) passos de comunicação</p>
<h3 id="radix-sort-em-paralelo"><a class="header" href="#radix-sort-em-paralelo"><em>Radix Sort</em> em Paralelo</a></h3>
<ul>
<li>Cada processador é responável por um subconjunto de valores dígitos;</li>
<li>Ordena e conta o número de valores dígitos;</li>
<li>Todos reduzem o número total de dígitos;</li>
<li>Envia as chaves para o processador responsável pelo intervalo do dígito;</li>
<li>Repete para o próximo dígito.</li>
</ul>
<p><strong>Complexidade</strong>:</p>
<ul>
<li>LSD - precisa de passos de comunicação equivalente ao número de dígitos a avaliar;</li>
<li>MSD - preciso de um passo de comunicação.</li>
</ul>
<h3 id="sampling-based"><a class="header" href="#sampling-based"><em>Sampling Based</em></a></h3>
<ul>
<li>Separa os dados em \( P \) conjuntos utilizando \( p - l \) <em>splitters</em>;</li>
<li>Cada processador age como um conjunto local;</li>
<li>Minimiza os movimentos de dados.</li>
</ul>
<h3 id="sampling-alternatives"><a class="header" href="#sampling-alternatives"><em>Sampling Alternatives</em></a></h3>
<ul>
<li><em>Regular Sampling</em> (\( p \times (p - l) \) chaves);
<ul>
<li>Não é efetivo para grandes \( p \).</li>
</ul>
</li>
<li><em>Random Sampling</em>;</li>
<li><em>Histogram Sampling</em>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="de-multicore-para-manycore"><a class="header" href="#de-multicore-para-manycore">De <em>Multicore</em> para <em>Manycore</em></a></h1>
<h2 id="problemas-chave"><a class="header" href="#problemas-chave">Problemas Chave</a></h2>
<ul>
<li><strong>Dicas para colocar múltiplos <em>cores</em> num único <em>chip</em></strong>:
<ul>
<li>Diminuir a capacidade computacional de cada <em>core</em>, mas não em demasia;</li>
<li>Utiliza uma rede de interconexão escalável no <em>chip</em> (NoC);
<ul>
<li>minimiza a latência de acesso à <em>cache</em>-partilhada e à memória-partilhada;</li>
<li>providencia largura de banda suficiente para a comunicação de dados;</li>
<li>minimiza os <em>bottlenecks</em> do tráfego.</li>
</ul>
</li>
<li>Agrupar <em>cores</em> em <em>clusters</em> de forma a melhorar a qualidade do NoC;</li>
<li>Reduzir o número de níveis e o tamanho da <em>cache</em> (é preciso ter em atenção ao impacto no desempenho);</li>
<li>Ter processos de fabrico mais pequenos;</li>
<li>Misturar PUs de propósito geral com módulos orientados à aplicação: GPUs para computação vetorial, TPU para <em>tensor computing</em>, ...</li>
<li>Mover para MCM ou <em>chiplets</em> (<em>chips</em> mais simples)</li>
</ul>
</li>
</ul>
<h2 id="fundamentos-da-interconexão"><a class="header" href="#fundamentos-da-interconexão">Fundamentos da Interconexão</a></h2>
<ul>
<li><em><strong>Networks-on-chip</strong></em>: um avanço nos sistemas de interconexão para conectar servidores em super-computadores;</li>
<li>Parâmetros chave para definir uma NoC:
<ul>
<li><strong>topologia</strong>: define como é que os nodos e os <em>links</em> estão conectados, nomeadamente todos os caminhos possíveis que uma mensagem poderá tomar pela rede;</li>
<li><strong>algoritmo de <em>routing</em></strong>: seleciona o caminho específico que uma mensagem deverá tomar desde a fonte até ao destino;</li>
<li><strong>protocolo de controlo de <em>flow</em></strong>: determina a forma como uma mensagem irá atravessar a rota que lhe foi atribuída;</li>
<li><em><strong>router micro architecture</strong></em>: implementa os protocolos de <em>routing</em> e de controlo de <em>flow</em>, bem como, de forma crítica, dá forma aos seus circuitos.</li>
</ul>
</li>
</ul>
<h2 id="pacotes-e-chips-manycore"><a class="header" href="#pacotes-e-chips-manycore">Pacotes e <em>Chips Manycore</em></a></h2>
<ul>
<li><strong>Intel</strong>: do <em>Intel MIC</em> para a família <em>Xeon Scalable</em>;</li>
<li><strong>AMD</strong>: família <em>Epyc Zen</em>;</li>
<li><strong>ARM</strong>: chaves <em>ARMv8</em> e <em>v9</em> competidores ao nível do servidor;
<ul>
<li>família <em>Marvell ThunderX</em>;</li>
<li><em>chip Fujitsu A64FX Arm</em>;</li>
<li>referência para o design <em>Neoverse</em> híper-escalável:
<ul>
<li><em>Ampere Altra Arm</em>;</li>
<li><em>Amazon Graviton</em>.</li>
</ul>
</li>
<li><em>Alibaba Yitian 710</em>;</li>
<li><em>Huawei HiSilicon Kunpeng 920</em>.</li>
</ul>
</li>
<li><strong>Sunway</strong>: família <em>SX260x0</em>;</li>
<li><strong>Cerebras</strong>: <em>Wafer Scale Engine</em>;</li>
<li><strong>Apple</strong> (não é um servidor): abordagem <em>SoC</em> (sem <em>chiplets</em>).
<ul>
<li>Defendem que é a melhor abordagem:
<ul>
<li>Menores latências;</li>
<li>Melhor processo de silicone (5nm);</li>
<li>Melhor <em>wafer fabrication</em>.</li>
</ul>
</li>
</ul>
</li>
<li><strong>NVidia</strong> (CPU+GPU): <em>Grace Hopper Superchip</em>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sistemas-top-hpc-nas-listas-do-top500"><a class="header" href="#sistemas-top-hpc-nas-listas-do-top500">Sistemas Top HPC nas listas do TOP500</a></h1>
<h2 id="hpc"><a class="header" href="#hpc">HPC</a></h2>
<p>Acrónimo para <em>High Performance Computing</em>.</p>
<h2 id="o-que-é-o-top500"><a class="header" href="#o-que-é-o-top500">O que é o TOP500</a></h2>
<p>É uma lista dos 500 sistemas de computação mais poderosos no mundo, sendo compilada pela organização TOP500.</p>
<p>Esta lista é baseada nos <em>benchmarks</em> LINPACK que mede a velocidade à qual um computador consegue resolver um denso sistema de equações lineares.</p>
<p>É muito conceituada e, por isso, é utilizada para seguir a evolução de tecnologia dos computadores e o aumento do poder dos sistemas de computação. É muito seguida na indústria da computação e, por vezes, utilizada por governos, instituições de pequisa e empresas para comparar os seus sistemas aos melhores do mundo.</p>
<h2 id="linpack-benchmarks-hpl"><a class="header" href="#linpack-benchmarks-hpl">LINPACK <em>Benchmarks</em> (HPL)</a></h2>
<p>Mede a velocidade com a qual um computador é capaz de resolver um sistema denso de \( n \) equações lineares do tipo \( Ax = b \).</p>
<p>O HPL, escrito em C, mede a taxa de <em>floating-point</em> sustentado (<em>GFLOPs/s</em>) para resolver um sistema linear de equações utilizando aritmética em <em>double-precision floating-point</em>.</p>
<h2 id="top500"><a class="header" href="#top500">TOP500</a></h2>
<h3 id="análise-de-sistemas-chave-em-2022"><a class="header" href="#análise-de-sistemas-chave-em-2022">Análise de Sistemas Chave em 2022</a></h3>
<ol>
<li><em>Frontier</em> (<em>AMD Epyc Trento 64c</em> + <em>AMD Instinct MI250x</em>);</li>
<li><em>Fugaku</em> (<em>Fujitsu A64FX</em>, 48 <em>cores</em>);</li>
<li><em>Leonardo</em> (<em>3ª Gen Xeon, 32c</em> + <em>NVidia Ampere A100</em>);</li>
<li><em>Summit</em> (<em>IBM POWER9</em>, 22 <em>cores</em> + <em>NVidia Volta GV100</em>) + <em>Sierra</em>;</li>
<li><em>TaihuLight</em> (<em>Sunway SW26010</em>, 260 <em>cores</em>);</li>
<li><em>Selene</em> (<em>AMD Epyc Rome 64c</em> + <em>NVidia A100</em>);</li>
<li><em>Tianhe-2A</em> (<em>MilkyWay-2A</em>) (<em>Xeon</em>, 12c + <em>Matrix-2000</em>).</li>
</ol>
<h2 id="o-que-é-o-green500"><a class="header" href="#o-que-é-o-green500">O que é o GREEN500?</a></h2>
<p>Lista dos 400 sistemas de computação ordenados com base na sua eficiência energética, tipicamente medida em <em>LINPACK FLOPS per Watt</em>.</p>
<h2 id="o-que-é-a-benchmark-hpcg"><a class="header" href="#o-que-é-a-benchmark-hpcg">O que é a <em>benchmark</em> HPCG?</a></h2>
<p>É um programa em C++ auto-contido com MPI e OpenMP que suporta medidas de desempenho em operações básicas num código unificado:</p>
<ul>
<li>Multiplicação de matrizes ou vetores dispersos;</li>
<li>Atualizações de vetores;</li>
<li><em>Dot Products</em> globais;</li>
<li><em>Local Symmetric Gauss-Seidl</em>;</li>
<li><em>Sparse triangular solve</em>.</li>
</ul>
<h2 id="o-que-é-o-hpl-ai"><a class="header" href="#o-que-é-o-hpl-ai">O que é o HPL-AI?</a></h2>
<p><em>High Performance Linpack for Artifical Intelligence</em> é uma <em>benchmark</em> para avaliar o desempenho de sistemas de computação em cargas de trabalho de <em>deep learning</em>. É baseado na <em>benchmark LINPACK</em>.</p>
<p>É uma ferramenta importante para identificar os sistemas mais poderosos para aplicação de inteligência artificial e para dar <em>track</em> ao progresso das tecnologias de computação nesta área.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
    </body>
</html>
